{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gymnasium as gym\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common import results_plotter\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.results_plotter import load_results, ts2xy\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveOnBestTrainingRewardCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    Callback for saving a model (the check is done every ``check_freq`` steps)\n",
    "    based on the training reward (in practice, we recommend using ``EvalCallback``).\n",
    "\n",
    "    :param check_freq: (int)\n",
    "    :param log_dir: (str) Path to the folder where the model will be saved.\n",
    "      It must contains the file created by the ``Monitor`` wrapper.\n",
    "    :param verbose: (int)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, check_freq: int, log_dir: str, verbose=1):\n",
    "        super().__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.log_dir = log_dir\n",
    "        self.save_path = os.path.join(log_dir, \"best_model\")\n",
    "        self.best_mean_reward = -np.inf\n",
    "\n",
    "    def _init_callback(self) -> None:\n",
    "        # Create folder if needed\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "\n",
    "            # Retrieve training reward\n",
    "            x, y = ts2xy(load_results(self.log_dir), \"timesteps\")\n",
    "            if len(x) > 0:\n",
    "                # Mean training reward over the last 100 episodes\n",
    "                mean_reward = np.mean(y[-100:])\n",
    "                if self.verbose > 0:\n",
    "                    print(f\"Num timesteps: {self.num_timesteps}\")\n",
    "                    print(\n",
    "                        f\"Best mean reward: {self.best_mean_reward:.2f} - Last mean reward per episode: {mean_reward:.2f}\"\n",
    "                    )\n",
    "\n",
    "                # New best model, you could save the agent here\n",
    "                if mean_reward > self.best_mean_reward:\n",
    "                    self.best_mean_reward = mean_reward\n",
    "                    # Example for saving best model\n",
    "                    if self.verbose > 0:\n",
    "                        print(f\"Saving new best model to {self.save_path}.zip\")\n",
    "                    self.model.save(self.save_path)\n",
    "\n",
    "        return True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(values, window):\n",
    "    \"\"\"\n",
    "    Smooth values by doing a moving average\n",
    "    :param values: (numpy array)\n",
    "    :param window: (int)\n",
    "    :return: (numpy array)\n",
    "    \"\"\"\n",
    "    weights = np.repeat(1.0, window) / window\n",
    "    return np.convolve(values, weights, \"valid\")\n",
    "\n",
    "\n",
    "def plot_results(log_folder, title=\"Learning Curve\"):\n",
    "    \"\"\"\n",
    "    plot the results\n",
    "\n",
    "    :param log_folder: (str) the save location of the results to plot\n",
    "    :param title: (str) the title of the task to plot\n",
    "    \"\"\"\n",
    "    x, y = ts2xy(load_results(log_folder), \"timesteps\")\n",
    "    #y = moving_average(y, window=50)\n",
    "    # Truncate x\n",
    "    #x = x[len(x) - len(y) :]\n",
    "\n",
    "    fig = plt.figure(title)\n",
    "    plt.plot(x, y)\n",
    "    plt.xlabel(\"Number of Timesteps\")\n",
    "    plt.ylabel(\"Rewards\")\n",
    "    plt.title(title + \" Smoothed\")\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment\n",
    "env = gym.make(\"CarRacing-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup callback\n",
    "\n",
    "# Create log dir\n",
    "log_dir = \"tmp/\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# Wrap the environment\n",
    "env = Monitor(env, log_dir)\n",
    "\n",
    "callback = SaveOnBestTrainingRewardCallback(check_freq=1000, log_dir=log_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "Num timesteps: 1000\n",
      "Best mean reward: -inf - Last mean reward per episode: -62.16\n",
      "Saving new best model to tmp/best_model.zip\n",
      "Num timesteps: 2000\n",
      "Best mean reward: -62.16 - Last mean reward per episode: -59.84\n",
      "Saving new best model to tmp/best_model.zip\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -59.8    |\n",
      "| time/              |          |\n",
      "|    fps             | 38       |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 52       |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "Num timesteps: 3000\n",
      "Best mean reward: -59.84 - Last mean reward per episode: -58.92\n",
      "Saving new best model to tmp/best_model.zip\n",
      "Num timesteps: 4000\n",
      "Best mean reward: -58.92 - Last mean reward per episode: -58.32\n",
      "Saving new best model to tmp/best_model.zip\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -58.3       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 30          |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 132         |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006254072 |\n",
      "|    clip_fraction        | 0.0449      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -4.24       |\n",
      "|    explained_variance   | -0.0353     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.228       |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00275    |\n",
      "|    std                  | 0.99        |\n",
      "|    value_loss           | 1.04        |\n",
      "-----------------------------------------\n",
      "Num timesteps: 5000\n",
      "Best mean reward: -58.32 - Last mean reward per episode: -57.10\n",
      "Saving new best model to tmp/best_model.zip\n",
      "Num timesteps: 6000\n",
      "Best mean reward: -57.10 - Last mean reward per episode: -55.86\n",
      "Saving new best model to tmp/best_model.zip\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -55.9        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 26           |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 228          |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0058882926 |\n",
      "|    clip_fraction        | 0.0588       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -4.21        |\n",
      "|    explained_variance   | 7.61e-05     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.32         |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.00455     |\n",
      "|    std                  | 0.98         |\n",
      "|    value_loss           | 0.769        |\n",
      "------------------------------------------\n",
      "Num timesteps: 7000\n",
      "Best mean reward: -55.86 - Last mean reward per episode: -56.43\n",
      "Num timesteps: 8000\n",
      "Best mean reward: -55.86 - Last mean reward per episode: -53.92\n",
      "Saving new best model to tmp/best_model.zip\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | -53.9     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 24        |\n",
      "|    iterations           | 4         |\n",
      "|    time_elapsed         | 334       |\n",
      "|    total_timesteps      | 8192      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0061021 |\n",
      "|    clip_fraction        | 0.0404    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -4.17     |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 0.448     |\n",
      "|    n_updates            | 30        |\n",
      "|    policy_gradient_loss | -0.00233  |\n",
      "|    std                  | 0.966     |\n",
      "|    value_loss           | 0.894     |\n",
      "---------------------------------------\n",
      "Num timesteps: 9000\n",
      "Best mean reward: -53.92 - Last mean reward per episode: -52.50\n",
      "Saving new best model to tmp/best_model.zip\n",
      "Num timesteps: 10000\n",
      "Best mean reward: -52.50 - Last mean reward per episode: -53.40\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -53.4        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 23           |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 433          |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0047936784 |\n",
      "|    clip_fraction        | 0.0377       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -4.14        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.411        |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.00292     |\n",
      "|    std                  | 0.959        |\n",
      "|    value_loss           | 0.947        |\n",
      "------------------------------------------\n",
      "Num timesteps: 11000\n",
      "Best mean reward: -52.50 - Last mean reward per episode: -51.01\n",
      "Saving new best model to tmp/best_model.zip\n",
      "Num timesteps: 12000\n",
      "Best mean reward: -51.01 - Last mean reward per episode: -49.54\n",
      "Saving new best model to tmp/best_model.zip\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -49.5       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 23          |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 534         |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008582542 |\n",
      "|    clip_fraction        | 0.103       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -4.1        |\n",
      "|    explained_variance   | 1.73e-06    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.472       |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.00874    |\n",
      "|    std                  | 0.944       |\n",
      "|    value_loss           | 0.973       |\n",
      "-----------------------------------------\n",
      "Num timesteps: 13000\n",
      "Best mean reward: -49.54 - Last mean reward per episode: -47.92\n",
      "Saving new best model to tmp/best_model.zip\n",
      "Num timesteps: 14000\n",
      "Best mean reward: -47.92 - Last mean reward per episode: -47.37\n",
      "Saving new best model to tmp/best_model.zip\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -47.4        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 22           |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 635          |\n",
      "|    total_timesteps      | 14336        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053116838 |\n",
      "|    clip_fraction        | 0.0553       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -4.07        |\n",
      "|    explained_variance   | 0.0072       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.393        |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | -0.00448     |\n",
      "|    std                  | 0.936        |\n",
      "|    value_loss           | 1.01         |\n",
      "------------------------------------------\n",
      "Num timesteps: 15000\n",
      "Best mean reward: -47.37 - Last mean reward per episode: -46.58\n",
      "Saving new best model to tmp/best_model.zip\n",
      "Num timesteps: 16000\n",
      "Best mean reward: -46.58 - Last mean reward per episode: -46.04\n",
      "Saving new best model to tmp/best_model.zip\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -46          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 22           |\n",
      "|    iterations           | 8            |\n",
      "|    time_elapsed         | 730          |\n",
      "|    total_timesteps      | 16384        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0058633694 |\n",
      "|    clip_fraction        | 0.0592       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -4.05        |\n",
      "|    explained_variance   | 0.00483      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.381        |\n",
      "|    n_updates            | 70           |\n",
      "|    policy_gradient_loss | -0.004       |\n",
      "|    std                  | 0.933        |\n",
      "|    value_loss           | 1.11         |\n",
      "------------------------------------------\n",
      "Num timesteps: 17000\n",
      "Best mean reward: -46.04 - Last mean reward per episode: -46.58\n",
      "Num timesteps: 18000\n",
      "Best mean reward: -46.04 - Last mean reward per episode: -46.10\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -46.1       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 22          |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 825         |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006197987 |\n",
      "|    clip_fraction        | 0.0664      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -4.04       |\n",
      "|    explained_variance   | 0.00627     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.696       |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.00505    |\n",
      "|    std                  | 0.93        |\n",
      "|    value_loss           | 1.47        |\n",
      "-----------------------------------------\n",
      "Num timesteps: 19000\n",
      "Best mean reward: -46.04 - Last mean reward per episode: -45.97\n",
      "Saving new best model to tmp/best_model.zip\n",
      "Num timesteps: 20000\n",
      "Best mean reward: -45.97 - Last mean reward per episode: -46.49\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -46.5       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 22          |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 922         |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006780885 |\n",
      "|    clip_fraction        | 0.057       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -4.02       |\n",
      "|    explained_variance   | 0.01        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.419       |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.00352    |\n",
      "|    std                  | 0.918       |\n",
      "|    value_loss           | 1.3         |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[39m# Train the agent and display a progress bar\u001b[39;00m\n\u001b[0;32m      4\u001b[0m timesteps \u001b[39m=\u001b[39m \u001b[39m90000\u001b[39m\n\u001b[1;32m----> 5\u001b[0m model\u001b[39m.\u001b[39;49mlearn(total_timesteps\u001b[39m=\u001b[39;49m\u001b[39mint\u001b[39;49m(timesteps), callback\u001b[39m=\u001b[39;49mcallback)\n",
      "File \u001b[1;32mc:\\Users\\candidomariz\\AppData\\Local\\miniconda3\\lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:308\u001b[0m, in \u001b[0;36mPPO.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    299\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlearn\u001b[39m(\n\u001b[0;32m    300\u001b[0m     \u001b[39mself\u001b[39m: SelfPPO,\n\u001b[0;32m    301\u001b[0m     total_timesteps: \u001b[39mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    306\u001b[0m     progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    307\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m SelfPPO:\n\u001b[1;32m--> 308\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mlearn(\n\u001b[0;32m    309\u001b[0m         total_timesteps\u001b[39m=\u001b[39;49mtotal_timesteps,\n\u001b[0;32m    310\u001b[0m         callback\u001b[39m=\u001b[39;49mcallback,\n\u001b[0;32m    311\u001b[0m         log_interval\u001b[39m=\u001b[39;49mlog_interval,\n\u001b[0;32m    312\u001b[0m         tb_log_name\u001b[39m=\u001b[39;49mtb_log_name,\n\u001b[0;32m    313\u001b[0m         reset_num_timesteps\u001b[39m=\u001b[39;49mreset_num_timesteps,\n\u001b[0;32m    314\u001b[0m         progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[0;32m    315\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\candidomariz\\AppData\\Local\\miniconda3\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:281\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    278\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlogger\u001b[39m.\u001b[39mrecord(\u001b[39m\"\u001b[39m\u001b[39mtime/total_timesteps\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps, exclude\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtensorboard\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    279\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlogger\u001b[39m.\u001b[39mdump(step\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps)\n\u001b[1;32m--> 281\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain()\n\u001b[0;32m    283\u001b[0m callback\u001b[39m.\u001b[39mon_training_end()\n\u001b[0;32m    285\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\candidomariz\\AppData\\Local\\miniconda3\\lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:272\u001b[0m, in \u001b[0;36mPPO.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    270\u001b[0m \u001b[39m# Optimization step\u001b[39;00m\n\u001b[0;32m    271\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpolicy\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m--> 272\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m    273\u001b[0m \u001b[39m# Clip grad norm\u001b[39;00m\n\u001b[0;32m    274\u001b[0m th\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mclip_grad_norm_(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpolicy\u001b[39m.\u001b[39mparameters(), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_grad_norm)\n",
      "File \u001b[1;32mc:\\Users\\candidomariz\\AppData\\Local\\miniconda3\\lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    489\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\candidomariz\\AppData\\Local\\miniconda3\\lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Instantiate the agent\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "# Train the agent and display a progress bar\n",
    "timesteps = 90000\n",
    "model.learn(total_timesteps=int(timesteps), callback=callback)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_reward, std_reward = evaluate_policy(model, model.get_env(), n_eval_episodes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-49.3209038 29.719247237129636\n"
     ]
    }
   ],
   "source": [
    "print(mean_reward, std_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index -1 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Helper from the library\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m results_plotter\u001b[39m.\u001b[39;49mplot_results(\n\u001b[0;32m      3\u001b[0m     [log_dir], timesteps, results_plotter\u001b[39m.\u001b[39;49mX_TIMESTEPS, \u001b[39m\"\u001b[39;49m\u001b[39mPPO CarRacing-v2\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[0;32m      4\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\candidomariz\\AppData\\Local\\miniconda3\\lib\\site-packages\\stable_baselines3\\common\\results_plotter.py:122\u001b[0m, in \u001b[0;36mplot_results\u001b[1;34m(dirs, num_timesteps, x_axis, task_name, figsize)\u001b[0m\n\u001b[0;32m    120\u001b[0m     data_frames\u001b[39m.\u001b[39mappend(data_frame)\n\u001b[0;32m    121\u001b[0m xy_list \u001b[39m=\u001b[39m [ts2xy(data_frame, x_axis) \u001b[39mfor\u001b[39;00m data_frame \u001b[39min\u001b[39;00m data_frames]\n\u001b[1;32m--> 122\u001b[0m plot_curves(xy_list, x_axis, task_name, figsize)\n",
      "File \u001b[1;32mc:\\Users\\candidomariz\\AppData\\Local\\miniconda3\\lib\\site-packages\\stable_baselines3\\common\\results_plotter.py:85\u001b[0m, in \u001b[0;36mplot_curves\u001b[1;34m(xy_list, x_axis, title, figsize)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \u001b[39mplot the curves\u001b[39;00m\n\u001b[0;32m     76\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[39m:param figsize: Size of the figure (width, height)\u001b[39;00m\n\u001b[0;32m     82\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     84\u001b[0m plt\u001b[39m.\u001b[39mfigure(title, figsize\u001b[39m=\u001b[39mfigsize)\n\u001b[1;32m---> 85\u001b[0m max_x \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39;49m(xy[\u001b[39m0\u001b[39;49m][\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m] \u001b[39mfor\u001b[39;49;00m xy \u001b[39min\u001b[39;49;00m xy_list)\n\u001b[0;32m     86\u001b[0m min_x \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m     87\u001b[0m \u001b[39mfor\u001b[39;00m _, (x, y) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(xy_list):\n",
      "File \u001b[1;32mc:\\Users\\candidomariz\\AppData\\Local\\miniconda3\\lib\\site-packages\\stable_baselines3\\common\\results_plotter.py:85\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \u001b[39mplot the curves\u001b[39;00m\n\u001b[0;32m     76\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[39m:param figsize: Size of the figure (width, height)\u001b[39;00m\n\u001b[0;32m     82\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     84\u001b[0m plt\u001b[39m.\u001b[39mfigure(title, figsize\u001b[39m=\u001b[39mfigsize)\n\u001b[1;32m---> 85\u001b[0m max_x \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(xy[\u001b[39m0\u001b[39;49m][\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m] \u001b[39mfor\u001b[39;00m xy \u001b[39min\u001b[39;00m xy_list)\n\u001b[0;32m     86\u001b[0m min_x \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m     87\u001b[0m \u001b[39mfor\u001b[39;00m _, (x, y) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(xy_list):\n",
      "\u001b[1;31mIndexError\u001b[0m: index -1 is out of bounds for axis 0 with size 0"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Helper from the library\n",
    "results_plotter.plot_results(\n",
    "    [log_dir], timesteps, results_plotter.X_TIMESTEPS, \"PPO CarRacing-v2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plot_results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m plot_results(log_dir)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plot_results' is not defined"
     ]
    }
   ],
   "source": [
    "plot_results(log_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    }
   ],
   "source": [
    "test_env = gym.make(\"CarRacing-v2\", render_mode='human')\n",
    "dir = log_dir + \"best_model\"\n",
    "model = PPO.load(\"ppo_car\", env=test_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_env = model.get_env()\n",
    "obs = vec_env.reset()\n",
    "while True:\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, done, info = vec_env.step(action)\n",
    "    #obs, rewards, dones, info = test_env.step(action)\n",
    "    vec_env.render()\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "vec_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    }
   ],
   "source": [
    "test_env = gym.make(\"CarRacing-v2\", render_mode='rgb_array')\n",
    "dir = log_dir + \"best_model\"\n",
    "model = PPO.load(\"ppo_car\", env=test_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    }
   ],
   "source": [
    "import imageio\n",
    "import numpy as np\n",
    "from stable_baselines3 import A2C\n",
    "\n",
    "model = A2C.load(\"ppo_car\", env=test_env)\n",
    "images = []\n",
    "\n",
    "vec_env = model.get_env()\n",
    "obs = vec_env.reset()\n",
    "#obs = model.env.reset()\n",
    "img = vec_env.render()\n",
    "for i in range(350):\n",
    "    images.append(img)\n",
    "    action, _ = model.predict(obs)\n",
    "    obs, _, _ ,_ = model.env.step(action)\n",
    "    img = vec_env.render(mode='rgb_array')\n",
    "\n",
    "imageio.mimsave('lander_a2c.gif', [np.array(img) for i, img in enumerate(images) if i%2 == 0], duration = 40)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discrete space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment\n",
    "env = gym.make(\"CarRacing-v2\", continuous=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -58      |\n",
      "| time/              |          |\n",
      "|    fps             | 29       |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 69       |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the agent\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "# Train the agent and display a progress bar\n",
    "model.learn(total_timesteps=int(1000))\n",
    "# Save the agent\n",
    "model.save(\"ppo_car_discrete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_reward, std_reward = evaluate_policy(model, model.get_env(), n_eval_episodes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-35.794415900000004 45.31521641763898\n"
     ]
    }
   ],
   "source": [
    "print(mean_reward, std_reward)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    }
   ],
   "source": [
    "test_env = gym.make(\"CarRacing-v2\", render_mode='human', continuous=False)\n",
    "model = PPO.load(\"ppo_car_discrete\", env=test_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_env = model.get_env()\n",
    "obs = vec_env.reset()\n",
    "while True:\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, done, info = vec_env.step(action)\n",
    "    #obs, rewards, dones, info = test_env.step(action)\n",
    "    vec_env.render()\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "vec_env.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lap completion percentage change "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment\n",
    "env = gym.make(\"CarRacing-v2\", lap_complete_percentage=0.5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the agent\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "# Train the agent and display a progress bar\n",
    "model.learn(total_timesteps=int(1000))\n",
    "# Save the agent\n",
    "model.save(\"ppo_car_reward\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_reward, std_reward = evaluate_policy(model, model.get_env(), n_eval_episodes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mean_reward, std_reward)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_env = gym.make(\"CarRacing-v2\", render_mode='human', continuous=False)\n",
    "model = PPO.load(\"ppo_car_reward\", env=test_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_env = model.get_env()\n",
    "obs = vec_env.reset()\n",
    "while True:\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, done, info = vec_env.step(action)\n",
    "    #obs, rewards, dones, info = test_env.step(action)\n",
    "    vec_env.render()\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "vec_env.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train with CNN instead of MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -52      |\n",
      "| time/              |          |\n",
      "|    fps             | 36       |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 55       |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -53.1       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 24          |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 164         |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009062252 |\n",
      "|    clip_fraction        | 0.118       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -4.26       |\n",
      "|    explained_variance   | 0.00824     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.136       |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00992    |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 0.723       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -50.7       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 22          |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 272         |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011202717 |\n",
      "|    clip_fraction        | 0.127       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -4.24       |\n",
      "|    explained_variance   | -0.00486    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.136       |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0114     |\n",
      "|    std                  | 0.988       |\n",
      "|    value_loss           | 0.426       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -48.4       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 21          |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 385         |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007981861 |\n",
      "|    clip_fraction        | 0.0718      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -4.2        |\n",
      "|    explained_variance   | 0.0994      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0796      |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00929    |\n",
      "|    std                  | 0.978       |\n",
      "|    value_loss           | 0.488       |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -48.6        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 20           |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 496          |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0065405304 |\n",
      "|    clip_fraction        | 0.0879       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -4.19        |\n",
      "|    explained_variance   | -0.125       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.217        |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.00786     |\n",
      "|    std                  | 0.978        |\n",
      "|    value_loss           | 0.667        |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -48.5       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 19          |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 617         |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011412453 |\n",
      "|    clip_fraction        | 0.125       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -4.18       |\n",
      "|    explained_variance   | 0.236       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.122       |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0125     |\n",
      "|    std                  | 0.972       |\n",
      "|    value_loss           | 0.456       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -50.2       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 19          |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 729         |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014542457 |\n",
      "|    clip_fraction        | 0.154       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -4.16       |\n",
      "|    explained_variance   | 0.0896      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0597      |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0161     |\n",
      "|    std                  | 0.966       |\n",
      "|    value_loss           | 0.421       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -51.1       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 19          |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 843         |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020185504 |\n",
      "|    clip_fraction        | 0.168       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -4.13       |\n",
      "|    explained_variance   | 0.198       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.11        |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0216     |\n",
      "|    std                  | 0.953       |\n",
      "|    value_loss           | 0.23        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -52.5       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 19          |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 962         |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018520031 |\n",
      "|    clip_fraction        | 0.169       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -4.11       |\n",
      "|    explained_variance   | 0.101       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0373      |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.018      |\n",
      "|    std                  | 0.952       |\n",
      "|    value_loss           | 0.231       |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1e+03      |\n",
      "|    ep_rew_mean          | -52.8      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 18         |\n",
      "|    iterations           | 10         |\n",
      "|    time_elapsed         | 1083       |\n",
      "|    total_timesteps      | 20480      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01847098 |\n",
      "|    clip_fraction        | 0.172      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -4.11      |\n",
      "|    explained_variance   | 0.365      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0581     |\n",
      "|    n_updates            | 90         |\n",
      "|    policy_gradient_loss | -0.0201    |\n",
      "|    std                  | 0.949      |\n",
      "|    value_loss           | 0.198      |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -52.9       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 18          |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 1198        |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027588366 |\n",
      "|    clip_fraction        | 0.227       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -4.08       |\n",
      "|    explained_variance   | 0.545       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0677      |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0238     |\n",
      "|    std                  | 0.937       |\n",
      "|    value_loss           | 0.241       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -53.4       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 18          |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 1324        |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020890083 |\n",
      "|    clip_fraction        | 0.188       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -4.05       |\n",
      "|    explained_variance   | 0.544       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0339      |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0224     |\n",
      "|    std                  | 0.932       |\n",
      "|    value_loss           | 0.219       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -54.6       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 18          |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 1438        |\n",
      "|    total_timesteps      | 26624       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022819063 |\n",
      "|    clip_fraction        | 0.216       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -4.02       |\n",
      "|    explained_variance   | 0.764       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.000643    |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0209     |\n",
      "|    std                  | 0.92        |\n",
      "|    value_loss           | 0.155       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -54.1       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 18          |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 1550        |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021446085 |\n",
      "|    clip_fraction        | 0.203       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.97       |\n",
      "|    explained_variance   | 0.792       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00895    |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0254     |\n",
      "|    std                  | 0.903       |\n",
      "|    value_loss           | 0.168       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -55.1       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 18          |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 1666        |\n",
      "|    total_timesteps      | 30720       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024260417 |\n",
      "|    clip_fraction        | 0.214       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.94       |\n",
      "|    explained_variance   | 0.826       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00765    |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0252     |\n",
      "|    std                  | 0.899       |\n",
      "|    value_loss           | 0.167       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -56.1       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 18          |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 1788        |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020192085 |\n",
      "|    clip_fraction        | 0.19        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.91       |\n",
      "|    explained_variance   | 0.908       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0219     |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.0195     |\n",
      "|    std                  | 0.884       |\n",
      "|    value_loss           | 0.0729      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -57         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 18          |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 1904        |\n",
      "|    total_timesteps      | 34816       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020135961 |\n",
      "|    clip_fraction        | 0.196       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.85       |\n",
      "|    explained_variance   | 0.906       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00641     |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0194     |\n",
      "|    std                  | 0.866       |\n",
      "|    value_loss           | 0.105       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -57.9       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 18          |\n",
      "|    iterations           | 18          |\n",
      "|    time_elapsed         | 2022        |\n",
      "|    total_timesteps      | 36864       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019370044 |\n",
      "|    clip_fraction        | 0.198       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.81       |\n",
      "|    explained_variance   | 0.799       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00661    |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.02       |\n",
      "|    std                  | 0.858       |\n",
      "|    value_loss           | 0.125       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -58.7       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 18          |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 2131        |\n",
      "|    total_timesteps      | 38912       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014215328 |\n",
      "|    clip_fraction        | 0.169       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.78       |\n",
      "|    explained_variance   | 0.877       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0113      |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.0179     |\n",
      "|    std                  | 0.85        |\n",
      "|    value_loss           | 0.104       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -59.4       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 18          |\n",
      "|    iterations           | 20          |\n",
      "|    time_elapsed         | 2252        |\n",
      "|    total_timesteps      | 40960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019990738 |\n",
      "|    clip_fraction        | 0.185       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.77       |\n",
      "|    explained_variance   | 0.956       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0382     |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.0222     |\n",
      "|    std                  | 0.85        |\n",
      "|    value_loss           | 0.074       |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1e+03      |\n",
      "|    ep_rew_mean          | -60.3      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 18         |\n",
      "|    iterations           | 21         |\n",
      "|    time_elapsed         | 2364       |\n",
      "|    total_timesteps      | 43008      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02960112 |\n",
      "|    clip_fraction        | 0.244      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -3.75      |\n",
      "|    explained_variance   | 0.955      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0264     |\n",
      "|    n_updates            | 200        |\n",
      "|    policy_gradient_loss | -0.018     |\n",
      "|    std                  | 0.843      |\n",
      "|    value_loss           | 0.0827     |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -60.8       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 18          |\n",
      "|    iterations           | 22          |\n",
      "|    time_elapsed         | 2478        |\n",
      "|    total_timesteps      | 45056       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019142209 |\n",
      "|    clip_fraction        | 0.221       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.74       |\n",
      "|    explained_variance   | 0.936       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0424     |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.0182     |\n",
      "|    std                  | 0.841       |\n",
      "|    value_loss           | 0.0649      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -61.2       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 18          |\n",
      "|    iterations           | 23          |\n",
      "|    time_elapsed         | 2572        |\n",
      "|    total_timesteps      | 47104       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019370764 |\n",
      "|    clip_fraction        | 0.182       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.72       |\n",
      "|    explained_variance   | 0.966       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0442     |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.0204     |\n",
      "|    std                  | 0.832       |\n",
      "|    value_loss           | 0.0654      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -61.7       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 18          |\n",
      "|    iterations           | 24          |\n",
      "|    time_elapsed         | 2671        |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026418244 |\n",
      "|    clip_fraction        | 0.201       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.69       |\n",
      "|    explained_variance   | 0.97        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0324     |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.0296     |\n",
      "|    std                  | 0.824       |\n",
      "|    value_loss           | 0.0651      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -62.2       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 18          |\n",
      "|    iterations           | 25          |\n",
      "|    time_elapsed         | 2777        |\n",
      "|    total_timesteps      | 51200       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027280636 |\n",
      "|    clip_fraction        | 0.216       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.68       |\n",
      "|    explained_variance   | 0.976       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0273     |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.0272     |\n",
      "|    std                  | 0.829       |\n",
      "|    value_loss           | 0.0514      |\n",
      "-----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the agent\n",
    "env = gym.make(\"CarRacing-v2\", render_mode = \"rgb_array\")\n",
    "model = PPO(\"CnnPolicy\", env, verbose=1)\n",
    "# Train the agent and display a progress bar\n",
    "timesteps = 50000\n",
    "model.learn(total_timesteps=int(timesteps), callback=callback)\n",
    "model.save(\"ppo_cnn\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-37.1440131 3.2612360342272515\n"
     ]
    }
   ],
   "source": [
    "mean_reward, std_reward = evaluate_policy(model, model.get_env(), n_eval_episodes=10)\n",
    "\n",
    "print(mean_reward, std_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gym' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m test_env \u001b[39m=\u001b[39m gym\u001b[39m.\u001b[39mmake(\u001b[39m\"\u001b[39m\u001b[39mCarRacing-v2\u001b[39m\u001b[39m\"\u001b[39m, render_mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mhuman\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      2\u001b[0m model \u001b[39m=\u001b[39m PPO\u001b[39m.\u001b[39mload(\u001b[39m\"\u001b[39m\u001b[39mppo_cnn\u001b[39m\u001b[39m\"\u001b[39m, env\u001b[39m=\u001b[39mtest_env)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'gym' is not defined"
     ]
    }
   ],
   "source": [
    "test_env = gym.make(\"CarRacing-v2\", render_mode='human')\n",
    "model = PPO.load(\"ppo_cnn\", env=test_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m vec_env \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mget_env()\n\u001b[0;32m      2\u001b[0m obs \u001b[39m=\u001b[39m vec_env\u001b[39m.\u001b[39mreset()\n\u001b[0;32m      3\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "vec_env = model.get_env()\n",
    "obs = vec_env.reset()\n",
    "while True:\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, done, info = vec_env.step(action)\n",
    "    #obs, rewards, dones, info = test_env.step(action)\n",
    "    vec_env.render()\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "vec_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index -1 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)\n",
      "Cell \u001b[1;32mIn[15], line 2\u001b[0m\n",
      "\u001b[0;32m      1\u001b[0m \u001b[39m# Helper from the library\u001b[39;00m\n",
      "\u001b[1;32m----> 2\u001b[0m results_plotter\u001b[39m.\u001b[39;49mplot_results(\n",
      "\u001b[0;32m      3\u001b[0m     [log_dir], timesteps, results_plotter\u001b[39m.\u001b[39;49mX_TIMESTEPS, \u001b[39m\"\u001b[39;49m\u001b[39mPPO CarRacing-v2\u001b[39;49m\u001b[39m\"\u001b[39;49m\n",
      "\u001b[0;32m      4\u001b[0m )\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\candidomariz\\AppData\\Local\\miniconda3\\lib\\site-packages\\stable_baselines3\\common\\results_plotter.py:122\u001b[0m, in \u001b[0;36mplot_results\u001b[1;34m(dirs, num_timesteps, x_axis, task_name, figsize)\u001b[0m\n",
      "\u001b[0;32m    120\u001b[0m     data_frames\u001b[39m.\u001b[39mappend(data_frame)\n",
      "\u001b[0;32m    121\u001b[0m xy_list \u001b[39m=\u001b[39m [ts2xy(data_frame, x_axis) \u001b[39mfor\u001b[39;00m data_frame \u001b[39min\u001b[39;00m data_frames]\n",
      "\u001b[1;32m--> 122\u001b[0m plot_curves(xy_list, x_axis, task_name, figsize)\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\candidomariz\\AppData\\Local\\miniconda3\\lib\\site-packages\\stable_baselines3\\common\\results_plotter.py:85\u001b[0m, in \u001b[0;36mplot_curves\u001b[1;34m(xy_list, x_axis, title, figsize)\u001b[0m\n",
      "\u001b[0;32m     74\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n",
      "\u001b[0;32m     75\u001b[0m \u001b[39mplot the curves\u001b[39;00m\n",
      "\u001b[0;32m     76\u001b[0m \n",
      "\u001b[1;32m   (...)\u001b[0m\n",
      "\u001b[0;32m     81\u001b[0m \u001b[39m:param figsize: Size of the figure (width, height)\u001b[39;00m\n",
      "\u001b[0;32m     82\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n",
      "\u001b[0;32m     84\u001b[0m plt\u001b[39m.\u001b[39mfigure(title, figsize\u001b[39m=\u001b[39mfigsize)\n",
      "\u001b[1;32m---> 85\u001b[0m max_x \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39;49m(xy[\u001b[39m0\u001b[39;49m][\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m] \u001b[39mfor\u001b[39;49;00m xy \u001b[39min\u001b[39;49;00m xy_list)\n",
      "\u001b[0;32m     86\u001b[0m min_x \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "\u001b[0;32m     87\u001b[0m \u001b[39mfor\u001b[39;00m _, (x, y) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(xy_list):\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\candidomariz\\AppData\\Local\\miniconda3\\lib\\site-packages\\stable_baselines3\\common\\results_plotter.py:85\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n",
      "\u001b[0;32m     74\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n",
      "\u001b[0;32m     75\u001b[0m \u001b[39mplot the curves\u001b[39;00m\n",
      "\u001b[0;32m     76\u001b[0m \n",
      "\u001b[1;32m   (...)\u001b[0m\n",
      "\u001b[0;32m     81\u001b[0m \u001b[39m:param figsize: Size of the figure (width, height)\u001b[39;00m\n",
      "\u001b[0;32m     82\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n",
      "\u001b[0;32m     84\u001b[0m plt\u001b[39m.\u001b[39mfigure(title, figsize\u001b[39m=\u001b[39mfigsize)\n",
      "\u001b[1;32m---> 85\u001b[0m max_x \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(xy[\u001b[39m0\u001b[39;49m][\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m] \u001b[39mfor\u001b[39;00m xy \u001b[39min\u001b[39;00m xy_list)\n",
      "\u001b[0;32m     86\u001b[0m min_x \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "\u001b[0;32m     87\u001b[0m \u001b[39mfor\u001b[39;00m _, (x, y) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(xy_list):\n",
      "\n",
      "\u001b[1;31mIndexError\u001b[0m: index -1 is out of bounds for axis 0 with size 0"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Helper from the library\n",
    "results_plotter.plot_results(\n",
    "    [log_dir], timesteps, results_plotter.X_TIMESTEPS, \"PPO CarRacing-v2\"\n",
    ")\n",
    "plot_results(log_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

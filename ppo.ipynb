{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gymnasium as gym\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common import results_plotter\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.results_plotter import load_results, ts2xy\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveOnBestTrainingRewardCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    Callback for saving a model (the check is done every ``check_freq`` steps)\n",
    "    based on the training reward (in practice, we recommend using ``EvalCallback``).\n",
    "\n",
    "    :param check_freq: (int)\n",
    "    :param log_dir: (str) Path to the folder where the model will be saved.\n",
    "      It must contains the file created by the ``Monitor`` wrapper.\n",
    "    :param verbose: (int)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, check_freq: int, log_dir: str, verbose=1):\n",
    "        super().__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.log_dir = log_dir\n",
    "        self.save_path = os.path.join(log_dir, \"best_model\")\n",
    "        self.best_mean_reward = -np.inf\n",
    "\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "\n",
    "            # Retrieve training reward\n",
    "            x, y = ts2xy(load_results(self.log_dir), \"timesteps\")\n",
    "            if len(x) > 0:\n",
    "                # Mean training reward over the last 100 episodes\n",
    "                mean_reward = np.mean(y[-100:])\n",
    "                if self.verbose > 0:\n",
    "                    print(f\"Num timesteps: {self.num_timesteps}\")\n",
    "                    print(\n",
    "                        f\"Best mean reward: {self.best_mean_reward:.2f} - Last mean reward per episode: {mean_reward:.2f}\"\n",
    "                    )\n",
    "\n",
    "                # New best model, you could save the agent here\n",
    "                if mean_reward > self.best_mean_reward:\n",
    "                    self.best_mean_reward = mean_reward\n",
    "                    # Example for saving best model\n",
    "                    if self.verbose > 0:\n",
    "                        print(f\"Saving new best model to {self.save_path}.zip\")\n",
    "                    self.model.save(self.save_path)\n",
    "\n",
    "        return True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(values, window):\n",
    "    \"\"\"\n",
    "    Smooth values by doing a moving average\n",
    "    :param values: (numpy array)\n",
    "    :param window: (int)\n",
    "    :return: (numpy array)\n",
    "    \"\"\"\n",
    "    weights = np.repeat(1.0, window) / window\n",
    "    return np.convolve(values, weights, \"valid\")\n",
    "\n",
    "\n",
    "def plot_results(log_folder, title=\"Learning Curve\"):\n",
    "    \"\"\"\n",
    "    plot the results\n",
    "\n",
    "    :param log_folder: (str) the save location of the results to plot\n",
    "    :param title: (str) the title of the task to plot\n",
    "    \"\"\"\n",
    "    x, y = ts2xy(load_results(log_folder), \"timesteps\")\n",
    "    #y = moving_average(y, window=50)\n",
    "    # Truncate x\n",
    "    #x = x[len(x) - len(y) :]\n",
    "\n",
    "    fig = plt.figure(title)\n",
    "    plt.plot(x, y)\n",
    "    plt.xlabel(\"Number of Timesteps\")\n",
    "    plt.ylabel(\"Rewards\")\n",
    "    plt.title(title + \" Smoothed\")\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Gif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "import numpy as np\n",
    "\n",
    "def create_gif(env, model_path, path, name):\n",
    "    model = PPO.load(model_path, env=env)\n",
    "    images = []\n",
    "\n",
    "    vec_env = model.get_env()\n",
    "    obs = vec_env.reset()\n",
    "    img = vec_env.render()\n",
    "    for i in range(500):\n",
    "        images.append(img)\n",
    "        action, _ = model.predict(obs)\n",
    "        obs, _, _ ,_ = vec_env.step(action)\n",
    "        img = vec_env.render(mode='rgb_array')\n",
    "    gif_name =  path + name + '.gif'\n",
    "    imageio.mimsave(gif_name, [np.array(img) for i, img in enumerate(images) if i%2 == 0], duration=120)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment\n",
    "env = gym.make(\"CarRacing-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup callback\n",
    "\n",
    "# Create log dir\n",
    "log_dir = \"ppo_normal/\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# Wrap the environment\n",
    "env = Monitor(env, log_dir)\n",
    "\n",
    "callback = SaveOnBestTrainingRewardCallback(check_freq=1000, log_dir=log_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "Num timesteps: 1000\n",
      "Best mean reward: -inf - Last mean reward per episode: -60.73\n",
      "Saving new best model to ppo_normal/best_model.zip\n",
      "Num timesteps: 2000\n",
      "Best mean reward: -60.73 - Last mean reward per episode: -57.07\n",
      "Saving new best model to ppo_normal/best_model.zip\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -57.1    |\n",
      "| time/              |          |\n",
      "|    fps             | 33       |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 60       |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "Num timesteps: 3000\n",
      "Best mean reward: -57.07 - Last mean reward per episode: -54.19\n",
      "Saving new best model to ppo_normal/best_model.zip\n",
      "Num timesteps: 4000\n",
      "Best mean reward: -54.19 - Last mean reward per episode: -54.71\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -54.7        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 22           |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 178          |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0077655218 |\n",
      "|    clip_fraction        | 0.0898       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -4.23        |\n",
      "|    explained_variance   | -0.029       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.442        |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.00694     |\n",
      "|    std                  | 0.983        |\n",
      "|    value_loss           | 1.27         |\n",
      "------------------------------------------\n",
      "Num timesteps: 5000\n",
      "Best mean reward: -54.19 - Last mean reward per episode: -52.17\n",
      "Saving new best model to ppo_normal/best_model.zip\n",
      "Num timesteps: 6000\n",
      "Best mean reward: -52.17 - Last mean reward per episode: -51.40\n",
      "Saving new best model to ppo_normal/best_model.zip\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -51.4       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 23          |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 258         |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007929702 |\n",
      "|    clip_fraction        | 0.0701      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -4.19       |\n",
      "|    explained_variance   | 0.00497     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.341       |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.00475    |\n",
      "|    std                  | 0.973       |\n",
      "|    value_loss           | 0.808       |\n",
      "-----------------------------------------\n",
      "Num timesteps: 7000\n",
      "Best mean reward: -51.40 - Last mean reward per episode: -51.93\n",
      "Num timesteps: 8000\n",
      "Best mean reward: -51.40 - Last mean reward per episode: -51.60\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -51.6       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 23          |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 355         |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006813377 |\n",
      "|    clip_fraction        | 0.0793      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -4.17       |\n",
      "|    explained_variance   | 0.00626     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.394       |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.006      |\n",
      "|    std                  | 0.968       |\n",
      "|    value_loss           | 0.869       |\n",
      "-----------------------------------------\n",
      "Num timesteps: 9000\n",
      "Best mean reward: -51.40 - Last mean reward per episode: -52.92\n",
      "Num timesteps: 10000\n",
      "Best mean reward: -51.40 - Last mean reward per episode: -53.39\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -53.4        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 22           |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 451          |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0068889535 |\n",
      "|    clip_fraction        | 0.0811       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -4.16        |\n",
      "|    explained_variance   | 0.00965      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.338        |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.00717     |\n",
      "|    std                  | 0.966        |\n",
      "|    value_loss           | 0.793        |\n",
      "------------------------------------------\n",
      "Num timesteps: 11000\n",
      "Best mean reward: -51.40 - Last mean reward per episode: -54.19\n",
      "Num timesteps: 12000\n",
      "Best mean reward: -51.40 - Last mean reward per episode: -52.41\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -52.4        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 22           |\n",
      "|    iterations           | 6            |\n",
      "|    time_elapsed         | 556          |\n",
      "|    total_timesteps      | 12288        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0077124196 |\n",
      "|    clip_fraction        | 0.0736       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -4.12        |\n",
      "|    explained_variance   | 0.0115       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.53         |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.00528     |\n",
      "|    std                  | 0.949        |\n",
      "|    value_loss           | 0.8          |\n",
      "------------------------------------------\n",
      "Num timesteps: 13000\n",
      "Best mean reward: -51.40 - Last mean reward per episode: -51.49\n",
      "Num timesteps: 14000\n",
      "Best mean reward: -51.40 - Last mean reward per episode: -49.51\n",
      "Saving new best model to ppo_normal/best_model.zip\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -49.5        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 21           |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 665          |\n",
      "|    total_timesteps      | 14336        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0049719075 |\n",
      "|    clip_fraction        | 0.0528       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -4.07        |\n",
      "|    explained_variance   | 0.0115       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.578        |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | -0.00295     |\n",
      "|    std                  | 0.931        |\n",
      "|    value_loss           | 1.02         |\n",
      "------------------------------------------\n",
      "Num timesteps: 15000\n",
      "Best mean reward: -49.51 - Last mean reward per episode: -50.37\n",
      "Num timesteps: 16000\n",
      "Best mean reward: -49.51 - Last mean reward per episode: -49.14\n",
      "Saving new best model to ppo_normal/best_model.zip\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -49.1        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 21           |\n",
      "|    iterations           | 8            |\n",
      "|    time_elapsed         | 759          |\n",
      "|    total_timesteps      | 16384        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0071468586 |\n",
      "|    clip_fraction        | 0.0661       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -4.02        |\n",
      "|    explained_variance   | 0.00437      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.774        |\n",
      "|    n_updates            | 70           |\n",
      "|    policy_gradient_loss | -0.00409     |\n",
      "|    std                  | 0.92         |\n",
      "|    value_loss           | 1.29         |\n",
      "------------------------------------------\n",
      "Num timesteps: 17000\n",
      "Best mean reward: -49.14 - Last mean reward per episode: -50.51\n",
      "Num timesteps: 18000\n",
      "Best mean reward: -49.14 - Last mean reward per episode: -50.82\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -50.8       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 21          |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 854         |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004173912 |\n",
      "|    clip_fraction        | 0.0322      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -4          |\n",
      "|    explained_variance   | 0.0166      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.501       |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.00112    |\n",
      "|    std                  | 0.917       |\n",
      "|    value_loss           | 1.13        |\n",
      "-----------------------------------------\n",
      "Num timesteps: 19000\n",
      "Best mean reward: -49.14 - Last mean reward per episode: -51.54\n",
      "Num timesteps: 20000\n",
      "Best mean reward: -49.14 - Last mean reward per episode: -47.57\n",
      "Saving new best model to ppo_normal/best_model.zip\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -47.6       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 21          |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 955         |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005850164 |\n",
      "|    clip_fraction        | 0.0383      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.97       |\n",
      "|    explained_variance   | 0.0327      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.497       |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.00311    |\n",
      "|    std                  | 0.905       |\n",
      "|    value_loss           | 0.999       |\n",
      "-----------------------------------------\n",
      "Num timesteps: 21000\n",
      "Best mean reward: -47.57 - Last mean reward per episode: -48.22\n",
      "Num timesteps: 22000\n",
      "Best mean reward: -47.57 - Last mean reward per episode: -47.26\n",
      "Saving new best model to ppo_normal/best_model.zip\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -47.3        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 21           |\n",
      "|    iterations           | 11           |\n",
      "|    time_elapsed         | 1048         |\n",
      "|    total_timesteps      | 22528        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0069337925 |\n",
      "|    clip_fraction        | 0.0582       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -3.93        |\n",
      "|    explained_variance   | 0.00391      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.891        |\n",
      "|    n_updates            | 100          |\n",
      "|    policy_gradient_loss | -0.00558     |\n",
      "|    std                  | 0.896        |\n",
      "|    value_loss           | 2.4          |\n",
      "------------------------------------------\n",
      "Num timesteps: 23000\n",
      "Best mean reward: -47.26 - Last mean reward per episode: -48.37\n",
      "Num timesteps: 24000\n",
      "Best mean reward: -47.26 - Last mean reward per episode: -48.83\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -48.8        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 21           |\n",
      "|    iterations           | 12           |\n",
      "|    time_elapsed         | 1142         |\n",
      "|    total_timesteps      | 24576        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042914306 |\n",
      "|    clip_fraction        | 0.0445       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -3.91        |\n",
      "|    explained_variance   | 0.00634      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.722        |\n",
      "|    n_updates            | 110          |\n",
      "|    policy_gradient_loss | -0.00218     |\n",
      "|    std                  | 0.892        |\n",
      "|    value_loss           | 1.78         |\n",
      "------------------------------------------\n",
      "Num timesteps: 25000\n",
      "Best mean reward: -47.26 - Last mean reward per episode: -44.42\n",
      "Saving new best model to ppo_normal/best_model.zip\n",
      "Num timesteps: 26000\n",
      "Best mean reward: -44.42 - Last mean reward per episode: -45.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -45          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 21           |\n",
      "|    iterations           | 13           |\n",
      "|    time_elapsed         | 1239         |\n",
      "|    total_timesteps      | 26624        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0070726667 |\n",
      "|    clip_fraction        | 0.0577       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -3.88        |\n",
      "|    explained_variance   | 0.0071       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.716        |\n",
      "|    n_updates            | 120          |\n",
      "|    policy_gradient_loss | -0.00311     |\n",
      "|    std                  | 0.881        |\n",
      "|    value_loss           | 2.28         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x20cb2e136d0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate the agent\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "# Train the agent and display a progress bar\n",
    "timesteps = 25000\n",
    "model.learn(total_timesteps=int(timesteps), callback=callback)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_reward, std_reward = evaluate_policy(model, model.get_env(), n_eval_episodes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-77.0105234 7.538367439023164\n"
     ]
    }
   ],
   "source": [
    "print(mean_reward, std_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper from the library\n",
    "#results_plotter.plot_results(\n",
    " #   [log_dir], timesteps, results_plotter.X_TIMESTEPS, \"PPO CarRacing-v2\"\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_results(log_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    }
   ],
   "source": [
    "test_env = gym.make(\"CarRacing-v2\", render_mode='human')\n",
    "dir = os.path.join(log_dir, \"best_model.zip\")\n",
    "model = PPO.load(dir, env=test_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_env = model.get_env()\n",
    "obs = vec_env.reset()\n",
    "while True:\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, done, info = vec_env.step(action)\n",
    "    #obs, rewards, dones, info = test_env.step(action)\n",
    "    vec_env.render()\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "vec_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    }
   ],
   "source": [
    "test_env = gym.make(\"CarRacing-v2\", render_mode='rgb_array')\n",
    "dir = os.path.join(log_dir, \"best_model\")\n",
    "create_gif(test_env, dir, log_dir, \"ppo_car\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discrete space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment\n",
    "env = gym.make(\"CarRacing-v2\", continuous=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup callback\n",
    "\n",
    "# Create log dir\n",
    "log_dir = \"ppo_discrete/\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# Wrap the environment\n",
    "env = Monitor(env, log_dir)\n",
    "\n",
    "callback = SaveOnBestTrainingRewardCallback(check_freq=1000, log_dir=log_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "Num timesteps: 1000\n",
      "Best mean reward: -inf - Last mean reward per episode: -61.65\n",
      "Saving new best model to ppo_discrete/best_model.zip\n",
      "Num timesteps: 2000\n",
      "Best mean reward: -61.65 - Last mean reward per episode: -56.35\n",
      "Saving new best model to ppo_discrete/best_model.zip\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -56.4    |\n",
      "| time/              |          |\n",
      "|    fps             | 40       |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 50       |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "Num timesteps: 3000\n",
      "Best mean reward: -56.35 - Last mean reward per episode: -53.29\n",
      "Saving new best model to ppo_discrete/best_model.zip\n",
      "Num timesteps: 4000\n",
      "Best mean reward: -53.29 - Last mean reward per episode: -47.80\n",
      "Saving new best model to ppo_discrete/best_model.zip\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -47.8       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 33          |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 121         |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012700672 |\n",
      "|    clip_fraction        | 0.138       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.6        |\n",
      "|    explained_variance   | -0.0177     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.394       |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0116     |\n",
      "|    value_loss           | 1.42        |\n",
      "-----------------------------------------\n",
      "Num timesteps: 5000\n",
      "Best mean reward: -47.80 - Last mean reward per episode: -42.69\n",
      "Saving new best model to ppo_discrete/best_model.zip\n",
      "Num timesteps: 6000\n",
      "Best mean reward: -42.69 - Last mean reward per episode: -38.86\n",
      "Saving new best model to ppo_discrete/best_model.zip\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -38.9       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 31          |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 196         |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010058181 |\n",
      "|    clip_fraction        | 0.106       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.58       |\n",
      "|    explained_variance   | 0.00814     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.484       |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0101     |\n",
      "|    value_loss           | 1.12        |\n",
      "-----------------------------------------\n",
      "Num timesteps: 7000\n",
      "Best mean reward: -38.86 - Last mean reward per episode: -35.01\n",
      "Saving new best model to ppo_discrete/best_model.zip\n",
      "Num timesteps: 8000\n",
      "Best mean reward: -35.01 - Last mean reward per episode: -33.84\n",
      "Saving new best model to ppo_discrete/best_model.zip\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1e+03      |\n",
      "|    ep_rew_mean          | -33.8      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 29         |\n",
      "|    iterations           | 4          |\n",
      "|    time_elapsed         | 278        |\n",
      "|    total_timesteps      | 8192       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01592116 |\n",
      "|    clip_fraction        | 0.144      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.54      |\n",
      "|    explained_variance   | 0.00712    |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.472      |\n",
      "|    n_updates            | 30         |\n",
      "|    policy_gradient_loss | -0.0129    |\n",
      "|    value_loss           | 1.08       |\n",
      "----------------------------------------\n",
      "Num timesteps: 9000\n",
      "Best mean reward: -33.84 - Last mean reward per episode: -32.89\n",
      "Saving new best model to ppo_discrete/best_model.zip\n",
      "Num timesteps: 10000\n",
      "Best mean reward: -32.89 - Last mean reward per episode: -32.37\n",
      "Saving new best model to ppo_discrete/best_model.zip\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -32.4        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 28           |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 357          |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0116657205 |\n",
      "|    clip_fraction        | 0.124        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.47        |\n",
      "|    explained_variance   | 0.00718      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.821        |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.011       |\n",
      "|    value_loss           | 2.45         |\n",
      "------------------------------------------\n",
      "Num timesteps: 11000\n",
      "Best mean reward: -32.37 - Last mean reward per episode: -30.75\n",
      "Saving new best model to ppo_discrete/best_model.zip\n",
      "Num timesteps: 12000\n",
      "Best mean reward: -30.75 - Last mean reward per episode: -31.03\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -31         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 28          |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 437         |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009045719 |\n",
      "|    clip_fraction        | 0.0727      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.4        |\n",
      "|    explained_variance   | 0.00664     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.44        |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.00392    |\n",
      "|    value_loss           | 3.21        |\n",
      "-----------------------------------------\n",
      "Num timesteps: 13000\n",
      "Best mean reward: -30.75 - Last mean reward per episode: -30.63\n",
      "Saving new best model to ppo_discrete/best_model.zip\n",
      "Num timesteps: 14000\n",
      "Best mean reward: -30.63 - Last mean reward per episode: -30.50\n",
      "Saving new best model to ppo_discrete/best_model.zip\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -30.5       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 27          |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 518         |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010882495 |\n",
      "|    clip_fraction        | 0.0941      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.32       |\n",
      "|    explained_variance   | 0.00612     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.61        |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.00725    |\n",
      "|    value_loss           | 4.31        |\n",
      "-----------------------------------------\n",
      "Num timesteps: 15000\n",
      "Best mean reward: -30.50 - Last mean reward per episode: -36.60\n",
      "Num timesteps: 16000\n",
      "Best mean reward: -30.50 - Last mean reward per episode: -33.12\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 996         |\n",
      "|    ep_rew_mean          | -33.1       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 27          |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 601         |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010859953 |\n",
      "|    clip_fraction        | 0.0887      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.27       |\n",
      "|    explained_variance   | 0.00583     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.37        |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0057     |\n",
      "|    value_loss           | 4.44        |\n",
      "-----------------------------------------\n",
      "Num timesteps: 17000\n",
      "Best mean reward: -30.50 - Last mean reward per episode: -37.62\n",
      "Num timesteps: 18000\n",
      "Best mean reward: -30.50 - Last mean reward per episode: -42.20\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 986          |\n",
      "|    ep_rew_mean          | -42.2        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 27           |\n",
      "|    iterations           | 9            |\n",
      "|    time_elapsed         | 682          |\n",
      "|    total_timesteps      | 18432        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0058744885 |\n",
      "|    clip_fraction        | 0.0563       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.33        |\n",
      "|    explained_variance   | -0.0844      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.26         |\n",
      "|    n_updates            | 80           |\n",
      "|    policy_gradient_loss | -0.00194     |\n",
      "|    value_loss           | 41           |\n",
      "------------------------------------------\n",
      "Num timesteps: 19000\n",
      "Best mean reward: -30.50 - Last mean reward per episode: -41.55\n",
      "Num timesteps: 20000\n",
      "Best mean reward: -30.50 - Last mean reward per episode: -42.02\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 968         |\n",
      "|    ep_rew_mean          | -40.9       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 26          |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 767         |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004762601 |\n",
      "|    clip_fraction        | 0.0342      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.26       |\n",
      "|    explained_variance   | -0.00254    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 59.7        |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0045     |\n",
      "|    value_loss           | 78          |\n",
      "-----------------------------------------\n",
      "Num timesteps: 21000\n",
      "Best mean reward: -30.50 - Last mean reward per episode: -40.92\n",
      "Num timesteps: 22000\n",
      "Best mean reward: -30.50 - Last mean reward per episode: -43.84\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 962         |\n",
      "|    ep_rew_mean          | -43         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 26          |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 852         |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004136525 |\n",
      "|    clip_fraction        | 0.0216      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.21       |\n",
      "|    explained_variance   | 0.162       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 21.5        |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | 0.00307     |\n",
      "|    value_loss           | 41.8        |\n",
      "-----------------------------------------\n",
      "Num timesteps: 23000\n",
      "Best mean reward: -30.50 - Last mean reward per episode: -45.36\n",
      "Num timesteps: 24000\n",
      "Best mean reward: -30.50 - Last mean reward per episode: -45.10\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 956          |\n",
      "|    ep_rew_mean          | -45.1        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 26           |\n",
      "|    iterations           | 12           |\n",
      "|    time_elapsed         | 940          |\n",
      "|    total_timesteps      | 24576        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048631397 |\n",
      "|    clip_fraction        | 0.0139       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.18        |\n",
      "|    explained_variance   | 0.249        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 48.6         |\n",
      "|    n_updates            | 110          |\n",
      "|    policy_gradient_loss | -0.00236     |\n",
      "|    value_loss           | 39           |\n",
      "------------------------------------------\n",
      "Num timesteps: 25000\n",
      "Best mean reward: -30.50 - Last mean reward per episode: -47.67\n",
      "Num timesteps: 26000\n",
      "Best mean reward: -30.50 - Last mean reward per episode: -49.44\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 930         |\n",
      "|    ep_rew_mean          | -50.4       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 25          |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 1030        |\n",
      "|    total_timesteps      | 26624       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008930741 |\n",
      "|    clip_fraction        | 0.0288      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.19       |\n",
      "|    explained_variance   | 0.416       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.01        |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.000576   |\n",
      "|    value_loss           | 41          |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x20cac178280>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate the agent\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "# Train the agent and display a progress bar\n",
    "timesteps = 25000\n",
    "model.learn(total_timesteps=int(timesteps), callback=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_reward, std_reward = evaluate_policy(model, model.get_env(), n_eval_episodes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-51.498120199999995 35.88498520079649\n"
     ]
    }
   ],
   "source": [
    "print(mean_reward, std_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper from the library\n",
    "#results_plotter.plot_results(\n",
    " #   [log_dir], timesteps, results_plotter.X_TIMESTEPS, \"PPO CarRacing-v2\"\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_results(log_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    }
   ],
   "source": [
    "test_env = gym.make(\"CarRacing-v2\", render_mode='human', continuous=False)\n",
    "dir = os.path.join(log_dir, \"best_model.zip\")\n",
    "model = PPO.load(dir, env=test_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_env = model.get_env()\n",
    "obs = vec_env.reset()\n",
    "while True:\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, done, info = vec_env.step(action)\n",
    "    #obs, rewards, dones, info = test_env.step(action)\n",
    "    vec_env.render()\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "vec_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    }
   ],
   "source": [
    "test_env = gym.make(\"CarRacing-v2\", render_mode='rgb_array', continuous=False)\n",
    "dir = os.path.join(log_dir, \"best_model\")\n",
    "create_gif(test_env, dir, log_dir, \"ppo_car\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lap completion percentage change "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment\n",
    "env = gym.make(\"CarRacing-v2\", lap_complete_percent=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup callback\n",
    "\n",
    "# Create log dir\n",
    "log_dir = \"ppo_lap/\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# Wrap the environment\n",
    "env = Monitor(env, log_dir)\n",
    "\n",
    "callback = SaveOnBestTrainingRewardCallback(check_freq=1000, log_dir=log_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "Num timesteps: 1000\n",
      "Best mean reward: -inf - Last mean reward per episode: -52.19\n",
      "Saving new best model to ppo_lap/best_model.zip\n",
      "Num timesteps: 2000\n",
      "Best mean reward: -52.19 - Last mean reward per episode: -54.20\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -54.2    |\n",
      "| time/              |          |\n",
      "|    fps             | 41       |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 49       |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "Num timesteps: 3000\n",
      "Best mean reward: -52.19 - Last mean reward per episode: -50.00\n",
      "Saving new best model to ppo_lap/best_model.zip\n",
      "Num timesteps: 4000\n",
      "Best mean reward: -50.00 - Last mean reward per episode: -48.77\n",
      "Saving new best model to ppo_lap/best_model.zip\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -48.8        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 35           |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 114          |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0097432565 |\n",
      "|    clip_fraction        | 0.13         |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -4.26        |\n",
      "|    explained_variance   | -0.00448     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.386        |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.0105      |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 1.47         |\n",
      "------------------------------------------\n",
      "Num timesteps: 5000\n",
      "Best mean reward: -48.77 - Last mean reward per episode: -52.12\n",
      "Num timesteps: 6000\n",
      "Best mean reward: -48.77 - Last mean reward per episode: -51.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -51         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 32          |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 190         |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008853113 |\n",
      "|    clip_fraction        | 0.124       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -4.24       |\n",
      "|    explained_variance   | 0.00411     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.528       |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.011      |\n",
      "|    std                  | 0.986       |\n",
      "|    value_loss           | 1.04        |\n",
      "-----------------------------------------\n",
      "Num timesteps: 7000\n",
      "Best mean reward: -48.77 - Last mean reward per episode: -50.88\n",
      "Num timesteps: 8000\n",
      "Best mean reward: -48.77 - Last mean reward per episode: -47.44\n",
      "Saving new best model to ppo_lap/best_model.zip\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -47.4       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 30          |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 269         |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008654518 |\n",
      "|    clip_fraction        | 0.0853      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -4.19       |\n",
      "|    explained_variance   | 0.00721     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.189       |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00624    |\n",
      "|    std                  | 0.972       |\n",
      "|    value_loss           | 0.703       |\n",
      "-----------------------------------------\n",
      "Num timesteps: 9000\n",
      "Best mean reward: -47.44 - Last mean reward per episode: -44.68\n",
      "Saving new best model to ppo_lap/best_model.zip\n",
      "Num timesteps: 10000\n",
      "Best mean reward: -44.68 - Last mean reward per episode: -44.05\n",
      "Saving new best model to ppo_lap/best_model.zip\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -44         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 29          |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 349         |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005312683 |\n",
      "|    clip_fraction        | 0.0439      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -4.16       |\n",
      "|    explained_variance   | 0.00779     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.766       |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.00178    |\n",
      "|    std                  | 0.966       |\n",
      "|    value_loss           | 1.05        |\n",
      "-----------------------------------------\n",
      "Num timesteps: 11000\n",
      "Best mean reward: -44.05 - Last mean reward per episode: -41.06\n",
      "Saving new best model to ppo_lap/best_model.zip\n",
      "Num timesteps: 12000\n",
      "Best mean reward: -41.06 - Last mean reward per episode: -41.09\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -41.1        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 28           |\n",
      "|    iterations           | 6            |\n",
      "|    time_elapsed         | 431          |\n",
      "|    total_timesteps      | 12288        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0070289895 |\n",
      "|    clip_fraction        | 0.0738       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -4.15        |\n",
      "|    explained_variance   | 0.0093       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.63         |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.00492     |\n",
      "|    std                  | 0.966        |\n",
      "|    value_loss           | 1.15         |\n",
      "------------------------------------------\n",
      "Num timesteps: 13000\n",
      "Best mean reward: -41.06 - Last mean reward per episode: -43.07\n",
      "Num timesteps: 14000\n",
      "Best mean reward: -41.06 - Last mean reward per episode: -41.84\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -41.8       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 27          |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 515         |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006862208 |\n",
      "|    clip_fraction        | 0.102       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -4.15       |\n",
      "|    explained_variance   | 0.0108      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.454       |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.00893    |\n",
      "|    std                  | 0.966       |\n",
      "|    value_loss           | 1.37        |\n",
      "-----------------------------------------\n",
      "Num timesteps: 15000\n",
      "Best mean reward: -41.06 - Last mean reward per episode: -42.73\n",
      "Num timesteps: 16000\n",
      "Best mean reward: -41.06 - Last mean reward per episode: -42.82\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -42.8       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 27          |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 595         |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006576048 |\n",
      "|    clip_fraction        | 0.0666      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -4.12       |\n",
      "|    explained_variance   | 0.0118      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.913       |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.00618    |\n",
      "|    std                  | 0.944       |\n",
      "|    value_loss           | 1.52        |\n",
      "-----------------------------------------\n",
      "Num timesteps: 17000\n",
      "Best mean reward: -41.06 - Last mean reward per episode: -41.91\n",
      "Num timesteps: 18000\n",
      "Best mean reward: -41.06 - Last mean reward per episode: -40.96\n",
      "Saving new best model to ppo_lap/best_model.zip\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -41         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 27          |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 677         |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005116478 |\n",
      "|    clip_fraction        | 0.0404      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -4.07       |\n",
      "|    explained_variance   | 0.00724     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.16        |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.00256    |\n",
      "|    std                  | 0.937       |\n",
      "|    value_loss           | 2.04        |\n",
      "-----------------------------------------\n",
      "Num timesteps: 19000\n",
      "Best mean reward: -40.96 - Last mean reward per episode: -40.54\n",
      "Saving new best model to ppo_lap/best_model.zip\n",
      "Num timesteps: 20000\n",
      "Best mean reward: -40.54 - Last mean reward per episode: -46.81\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 994          |\n",
      "|    ep_rew_mean          | -46.8        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 26           |\n",
      "|    iterations           | 10           |\n",
      "|    time_elapsed         | 761          |\n",
      "|    total_timesteps      | 20480        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053892843 |\n",
      "|    clip_fraction        | 0.0527       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -4.04        |\n",
      "|    explained_variance   | 0.00814      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.852        |\n",
      "|    n_updates            | 90           |\n",
      "|    policy_gradient_loss | -0.00279     |\n",
      "|    std                  | 0.923        |\n",
      "|    value_loss           | 2.28         |\n",
      "------------------------------------------\n",
      "Num timesteps: 21000\n",
      "Best mean reward: -40.54 - Last mean reward per episode: -50.44\n",
      "Num timesteps: 22000\n",
      "Best mean reward: -40.54 - Last mean reward per episode: -54.46\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 984          |\n",
      "|    ep_rew_mean          | -54.5        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 26           |\n",
      "|    iterations           | 11           |\n",
      "|    time_elapsed         | 848          |\n",
      "|    total_timesteps      | 22528        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025545743 |\n",
      "|    clip_fraction        | 0.0177       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -4.01        |\n",
      "|    explained_variance   | -0.0545      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 37.5         |\n",
      "|    n_updates            | 100          |\n",
      "|    policy_gradient_loss | 0.00127      |\n",
      "|    std                  | 0.919        |\n",
      "|    value_loss           | 35.2         |\n",
      "------------------------------------------\n",
      "Num timesteps: 23000\n",
      "Best mean reward: -40.54 - Last mean reward per episode: -54.32\n",
      "Num timesteps: 24000\n",
      "Best mean reward: -40.54 - Last mean reward per episode: -54.61\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 986          |\n",
      "|    ep_rew_mean          | -54.6        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 26           |\n",
      "|    iterations           | 12           |\n",
      "|    time_elapsed         | 937          |\n",
      "|    total_timesteps      | 24576        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053643254 |\n",
      "|    clip_fraction        | 0.0205       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -4           |\n",
      "|    explained_variance   | 0.151        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 13.5         |\n",
      "|    n_updates            | 110          |\n",
      "|    policy_gradient_loss | -0.00221     |\n",
      "|    std                  | 0.917        |\n",
      "|    value_loss           | 58           |\n",
      "------------------------------------------\n",
      "Num timesteps: 25000\n",
      "Best mean reward: -40.54 - Last mean reward per episode: -54.09\n",
      "Num timesteps: 26000\n",
      "Best mean reward: -40.54 - Last mean reward per episode: -53.62\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 987          |\n",
      "|    ep_rew_mean          | -53.6        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 25           |\n",
      "|    iterations           | 13           |\n",
      "|    time_elapsed         | 1026         |\n",
      "|    total_timesteps      | 26624        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056846086 |\n",
      "|    clip_fraction        | 0.0487       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -4           |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.02         |\n",
      "|    n_updates            | 120          |\n",
      "|    policy_gradient_loss | -0.00271     |\n",
      "|    std                  | 0.917        |\n",
      "|    value_loss           | 2.28         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x20caf561d00>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate the agent\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "# Train the agent and display a progress bar\n",
    "timesteps = 25000\n",
    "model.learn(total_timesteps=int(timesteps), callback=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_reward, std_reward = evaluate_policy(model, model.get_env(), n_eval_episodes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-69.51982840000001 1.939266231595713\n"
     ]
    }
   ],
   "source": [
    "print(mean_reward, std_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper from the library\n",
    "#results_plotter.plot_results(\n",
    " #   [log_dir], timesteps, results_plotter.X_TIMESTEPS, \"PPO CarRacing-v2\"\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_results(log_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    }
   ],
   "source": [
    "test_env = gym.make(\"CarRacing-v2\", render_mode='human')#, continuous=False)\n",
    "dir = os.path.join(log_dir, \"best_model.zip\")\n",
    "model = PPO.load(dir, env=test_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_env = model.get_env()\n",
    "obs = vec_env.reset()\n",
    "while True:\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, done, info = vec_env.step(action)\n",
    "    #obs, rewards, dones, info = test_env.step(action)\n",
    "    vec_env.render()\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "vec_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    }
   ],
   "source": [
    "test_env = gym.make(\"CarRacing-v2\", render_mode='rgb_array')\n",
    "dir = os.path.join(log_dir, \"best_model\")\n",
    "create_gif(test_env, dir, log_dir, \"ppo_car\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train with CNN instead of MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CarRacing-v2\", render_mode = \"rgb_array\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup callback\n",
    "\n",
    "# Create log dir\n",
    "log_dir = \"ppo_cnn/\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# Wrap the environment\n",
    "env = Monitor(env, log_dir)\n",
    "\n",
    "callback = SaveOnBestTrainingRewardCallback(check_freq=1000, log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "Num timesteps: 1000\n",
      "Best mean reward: -inf - Last mean reward per episode: -55.93\n",
      "Saving new best model to ppo_cnn/best_model.zip\n",
      "Num timesteps: 2000\n",
      "Best mean reward: -55.93 - Last mean reward per episode: -58.79\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -58.8    |\n",
      "| time/              |          |\n",
      "|    fps             | 42       |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 47       |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "Num timesteps: 3000\n",
      "Best mean reward: -55.93 - Last mean reward per episode: -56.49\n",
      "Num timesteps: 4000\n",
      "Best mean reward: -55.93 - Last mean reward per episode: -54.53\n",
      "Saving new best model to ppo_cnn/best_model.zip\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -54.5        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 28           |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 144          |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0060140323 |\n",
      "|    clip_fraction        | 0.0796       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -4.25        |\n",
      "|    explained_variance   | 0.00487      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.207        |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.00624     |\n",
      "|    std                  | 0.994        |\n",
      "|    value_loss           | 0.527        |\n",
      "------------------------------------------\n",
      "Num timesteps: 5000\n",
      "Best mean reward: -54.53 - Last mean reward per episode: -51.55\n",
      "Saving new best model to ppo_cnn/best_model.zip\n",
      "Num timesteps: 6000\n",
      "Best mean reward: -51.55 - Last mean reward per episode: -51.45\n",
      "Saving new best model to ppo_cnn/best_model.zip\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -51.5       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 24          |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 246         |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008640703 |\n",
      "|    clip_fraction        | 0.111       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -4.24       |\n",
      "|    explained_variance   | 0.0278      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.19        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0115     |\n",
      "|    std                  | 0.992       |\n",
      "|    value_loss           | 0.54        |\n",
      "-----------------------------------------\n",
      "Num timesteps: 7000\n",
      "Best mean reward: -51.45 - Last mean reward per episode: -52.65\n",
      "Num timesteps: 8000\n",
      "Best mean reward: -51.45 - Last mean reward per episode: -50.27\n",
      "Saving new best model to ppo_cnn/best_model.zip\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -50.3       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 23          |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 346         |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010354636 |\n",
      "|    clip_fraction        | 0.117       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -4.22       |\n",
      "|    explained_variance   | 0.143       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.142       |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0122     |\n",
      "|    std                  | 0.986       |\n",
      "|    value_loss           | 0.532       |\n",
      "-----------------------------------------\n",
      "Num timesteps: 9000\n",
      "Best mean reward: -50.27 - Last mean reward per episode: -51.26\n",
      "Num timesteps: 10000\n",
      "Best mean reward: -50.27 - Last mean reward per episode: -48.47\n",
      "Saving new best model to ppo_cnn/best_model.zip\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -48.5       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 22          |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 447         |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008425316 |\n",
      "|    clip_fraction        | 0.109       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -4.2        |\n",
      "|    explained_variance   | 0.0392      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0753      |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.00964    |\n",
      "|    std                  | 0.98        |\n",
      "|    value_loss           | 0.429       |\n",
      "-----------------------------------------\n",
      "Num timesteps: 11000\n",
      "Best mean reward: -48.47 - Last mean reward per episode: -45.76\n",
      "Saving new best model to ppo_cnn/best_model.zip\n",
      "Num timesteps: 12000\n",
      "Best mean reward: -45.76 - Last mean reward per episode: -44.05\n",
      "Saving new best model to ppo_cnn/best_model.zip\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -44         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 22          |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 546         |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009858774 |\n",
      "|    clip_fraction        | 0.102       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -4.19       |\n",
      "|    explained_variance   | 0.184       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.152       |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0126     |\n",
      "|    std                  | 0.978       |\n",
      "|    value_loss           | 0.473       |\n",
      "-----------------------------------------\n",
      "Num timesteps: 13000\n",
      "Best mean reward: -44.05 - Last mean reward per episode: -44.40\n",
      "Num timesteps: 14000\n",
      "Best mean reward: -44.05 - Last mean reward per episode: -46.49\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -46.5       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 21          |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 663         |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007661966 |\n",
      "|    clip_fraction        | 0.0929      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -4.19       |\n",
      "|    explained_variance   | 0.326       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.109       |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0111     |\n",
      "|    std                  | 0.981       |\n",
      "|    value_loss           | 0.574       |\n",
      "-----------------------------------------\n",
      "Num timesteps: 15000\n",
      "Best mean reward: -44.05 - Last mean reward per episode: -43.97\n",
      "Saving new best model to ppo_cnn/best_model.zip\n",
      "Num timesteps: 16000\n",
      "Best mean reward: -43.97 - Last mean reward per episode: -44.13\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -44.1       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 19          |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 824         |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015694614 |\n",
      "|    clip_fraction        | 0.138       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -4.21       |\n",
      "|    explained_variance   | 0.275       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.109       |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0156     |\n",
      "|    std                  | 0.985       |\n",
      "|    value_loss           | 0.252       |\n",
      "-----------------------------------------\n",
      "Num timesteps: 17000\n",
      "Best mean reward: -43.97 - Last mean reward per episode: -41.35\n",
      "Saving new best model to ppo_cnn/best_model.zip\n",
      "Num timesteps: 18000\n",
      "Best mean reward: -41.35 - Last mean reward per episode: -40.41\n",
      "Saving new best model to ppo_cnn/best_model.zip\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -40.4       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 18          |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 1007        |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016786747 |\n",
      "|    clip_fraction        | 0.199       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -4.19       |\n",
      "|    explained_variance   | 0.308       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0284      |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0252     |\n",
      "|    std                  | 0.971       |\n",
      "|    value_loss           | 0.335       |\n",
      "-----------------------------------------\n",
      "Num timesteps: 19000\n",
      "Best mean reward: -40.41 - Last mean reward per episode: -42.33\n",
      "Num timesteps: 20000\n",
      "Best mean reward: -40.41 - Last mean reward per episode: -41.76\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -41.8        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 18           |\n",
      "|    iterations           | 10           |\n",
      "|    time_elapsed         | 1124         |\n",
      "|    total_timesteps      | 20480        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0143506415 |\n",
      "|    clip_fraction        | 0.165        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -4.15        |\n",
      "|    explained_variance   | 0.421        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0418       |\n",
      "|    n_updates            | 90           |\n",
      "|    policy_gradient_loss | -0.0175      |\n",
      "|    std                  | 0.962        |\n",
      "|    value_loss           | 0.356        |\n",
      "------------------------------------------\n",
      "Num timesteps: 21000\n",
      "Best mean reward: -40.41 - Last mean reward per episode: -41.26\n",
      "Num timesteps: 22000\n",
      "Best mean reward: -40.41 - Last mean reward per episode: -40.67\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -40.7       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 18          |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 1227        |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019598745 |\n",
      "|    clip_fraction        | 0.176       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -4.12       |\n",
      "|    explained_variance   | 0.65        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0112      |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0194     |\n",
      "|    std                  | 0.951       |\n",
      "|    value_loss           | 0.243       |\n",
      "-----------------------------------------\n",
      "Num timesteps: 23000\n",
      "Best mean reward: -40.41 - Last mean reward per episode: -40.28\n",
      "Saving new best model to ppo_cnn/best_model.zip\n",
      "Num timesteps: 24000\n",
      "Best mean reward: -40.28 - Last mean reward per episode: -41.15\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -41.2       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 18          |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 1332        |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015759196 |\n",
      "|    clip_fraction        | 0.161       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -4.08       |\n",
      "|    explained_variance   | 0.751       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0374      |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0194     |\n",
      "|    std                  | 0.937       |\n",
      "|    value_loss           | 0.266       |\n",
      "-----------------------------------------\n",
      "Num timesteps: 25000\n",
      "Best mean reward: -40.28 - Last mean reward per episode: -38.85\n",
      "Saving new best model to ppo_cnn/best_model.zip\n",
      "Num timesteps: 26000\n",
      "Best mean reward: -38.85 - Last mean reward per episode: -38.16\n",
      "Saving new best model to ppo_cnn/best_model.zip\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -38.2       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 18          |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 1442        |\n",
      "|    total_timesteps      | 26624       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013914768 |\n",
      "|    clip_fraction        | 0.133       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -4.04       |\n",
      "|    explained_variance   | 0.863       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0838      |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0146     |\n",
      "|    std                  | 0.927       |\n",
      "|    value_loss           | 0.364       |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x20cafbc0d60>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate the agent\n",
    "model = PPO(\"CnnPolicy\", env, verbose=1)\n",
    "# Train the agent and display a progress bar\n",
    "timesteps = 25000\n",
    "model.learn(total_timesteps=int(timesteps), callback=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-51.159724 24.48586646563704\n"
     ]
    }
   ],
   "source": [
    "mean_reward, std_reward = evaluate_policy(model, model.get_env(), n_eval_episodes=5)\n",
    "\n",
    "print(mean_reward, std_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper from the library\n",
    "#results_plotter.plot_results(\n",
    " #   [log_dir], timesteps, results_plotter.X_TIMESTEPS, \"PPO CarRacing-v2\"\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_results(log_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    }
   ],
   "source": [
    "test_env = gym.make(\"CarRacing-v2\", render_mode='human')\n",
    "dir = os.path.join(log_dir, \"best_model.zip\")\n",
    "model = PPO.load(dir, env=test_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_env = model.get_env()\n",
    "obs = vec_env.reset()\n",
    "while True:\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, done, info = vec_env.step(action)\n",
    "    #obs, rewards, dones, info = test_env.step(action)\n",
    "    vec_env.render()\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "vec_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    }
   ],
   "source": [
    "test_env = gym.make(\"CarRacing-v2\", render_mode='rgb_array')\n",
    "dir = os.path.join(log_dir, \"best_model\")\n",
    "create_gif(test_env, dir, log_dir, \"ppo_car\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

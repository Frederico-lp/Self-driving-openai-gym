{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gymnasium as gym\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common import results_plotter\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.results_plotter import load_results, ts2xy\n",
    "from stable_baselines3.common.monitor import Monitor\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveOnBestTrainingRewardCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    Callback for saving a model (the check is done every ``check_freq`` steps)\n",
    "    based on the training reward (in practice, we recommend using ``EvalCallback``).\n",
    "\n",
    "    :param check_freq: (int)\n",
    "    :param log_dir: (str) Path to the folder where the model will be saved.\n",
    "      It must contains the file created by the ``Monitor`` wrapper.\n",
    "    :param verbose: (int)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, check_freq: int, log_dir: str, verbose=1):\n",
    "        super().__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.log_dir = log_dir\n",
    "        self.save_path = os.path.join(log_dir, \"best_model\")\n",
    "        self.best_mean_reward = -np.inf\n",
    "\n",
    "    def _init_callback(self) -> None:\n",
    "        # Create folder if needed\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "\n",
    "            # Retrieve training reward\n",
    "            x, y = ts2xy(load_results(self.log_dir), \"timesteps\")\n",
    "            if len(x) > 0:\n",
    "                # Mean training reward over the last 100 episodes\n",
    "                mean_reward = np.mean(y[-100:])\n",
    "                if self.verbose > 0:\n",
    "                    print(f\"Num timesteps: {self.num_timesteps}\")\n",
    "                    print(\n",
    "                        f\"Best mean reward: {self.best_mean_reward:.2f} - Last mean reward per episode: {mean_reward:.2f}\"\n",
    "                    )\n",
    "\n",
    "                # New best model, you could save the agent here\n",
    "                if mean_reward > self.best_mean_reward:\n",
    "                    self.best_mean_reward = mean_reward\n",
    "                    # Example for saving best model\n",
    "                    if self.verbose > 0:\n",
    "                        print(f\"Saving new best model to {self.save_path}.zip\")\n",
    "                    self.model.save(self.save_path)\n",
    "\n",
    "        return True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(values, window):\n",
    "    \"\"\"\n",
    "    Smooth values by doing a moving average\n",
    "    :param values: (numpy array)\n",
    "    :param window: (int)\n",
    "    :return: (numpy array)\n",
    "    \"\"\"\n",
    "    weights = np.repeat(1.0, window) / window\n",
    "    return np.convolve(values, weights, \"valid\")\n",
    "\n",
    "\n",
    "def plot_results(log_folder, title=\"Learning Curve\"):\n",
    "    \"\"\"\n",
    "    plot the results\n",
    "\n",
    "    :param log_folder: (str) the save location of the results to plot\n",
    "    :param title: (str) the title of the task to plot\n",
    "    \"\"\"\n",
    "    x, y = ts2xy(load_results(log_folder), \"timesteps\")\n",
    "    #y = moving_average(y, window=50)\n",
    "    # Truncate x\n",
    "    #x = x[len(x) - len(y) :]\n",
    "\n",
    "    fig = plt.figure(title)\n",
    "    plt.plot(x, y)\n",
    "    plt.xlabel(\"Number of Timesteps\")\n",
    "    plt.ylabel(\"Rewards\")\n",
    "    plt.title(title + \" Smoothed\")\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Gif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "import numpy as np\n",
    "from stable_baselines3 import A2C\n",
    "\n",
    "def create_gif(env, model_path, path, name):\n",
    "    model = A2C.load(model_path, env=env)\n",
    "    images = []\n",
    "\n",
    "    vec_env = model.get_env()\n",
    "    obs = vec_env.reset()\n",
    "    img = vec_env.render()\n",
    "    for i in range(500):\n",
    "        images.append(img)\n",
    "        action, _ = model.predict(obs)\n",
    "        obs, _, _ ,_ = vec_env.step(action)\n",
    "        img = vec_env.render(mode='rgb_array')\n",
    "    gif_name =  path + name + '.gif'\n",
    "    imageio.mimsave(gif_name, [np.array(img) for i, img in enumerate(images) if i%2 == 0], fps=29)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment\n",
    "env = gym.make(\"CarRacing-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup callback\n",
    "\n",
    "# Create log dir\n",
    "log_dir = \"tmp/\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# Wrap the environment\n",
    "env = Monitor(env, log_dir)\n",
    "\n",
    "callback = SaveOnBestTrainingRewardCallback(check_freq=1000, log_dir=log_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "Num timesteps: 1000\n",
      "Best mean reward: -inf - Last mean reward per episode: -62.16\n",
      "Saving new best model to tmp/best_model.zip\n",
      "Num timesteps: 2000\n",
      "Best mean reward: -62.16 - Last mean reward per episode: -59.84\n",
      "Saving new best model to tmp/best_model.zip\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -59.8    |\n",
      "| time/              |          |\n",
      "|    fps             | 38       |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 52       |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "Num timesteps: 3000\n",
      "Best mean reward: -59.84 - Last mean reward per episode: -58.92\n",
      "Saving new best model to tmp/best_model.zip\n",
      "Num timesteps: 4000\n",
      "Best mean reward: -58.92 - Last mean reward per episode: -58.32\n",
      "Saving new best model to tmp/best_model.zip\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -58.3       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 30          |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 132         |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006254072 |\n",
      "|    clip_fraction        | 0.0449      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -4.24       |\n",
      "|    explained_variance   | -0.0353     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.228       |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00275    |\n",
      "|    std                  | 0.99        |\n",
      "|    value_loss           | 1.04        |\n",
      "-----------------------------------------\n",
      "Num timesteps: 5000\n",
      "Best mean reward: -58.32 - Last mean reward per episode: -57.10\n",
      "Saving new best model to tmp/best_model.zip\n",
      "Num timesteps: 6000\n",
      "Best mean reward: -57.10 - Last mean reward per episode: -55.86\n",
      "Saving new best model to tmp/best_model.zip\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -55.9        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 26           |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 228          |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0058882926 |\n",
      "|    clip_fraction        | 0.0588       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -4.21        |\n",
      "|    explained_variance   | 7.61e-05     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.32         |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.00455     |\n",
      "|    std                  | 0.98         |\n",
      "|    value_loss           | 0.769        |\n",
      "------------------------------------------\n",
      "Num timesteps: 7000\n",
      "Best mean reward: -55.86 - Last mean reward per episode: -56.43\n",
      "Num timesteps: 8000\n",
      "Best mean reward: -55.86 - Last mean reward per episode: -53.92\n",
      "Saving new best model to tmp/best_model.zip\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | -53.9     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 24        |\n",
      "|    iterations           | 4         |\n",
      "|    time_elapsed         | 334       |\n",
      "|    total_timesteps      | 8192      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0061021 |\n",
      "|    clip_fraction        | 0.0404    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -4.17     |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 0.448     |\n",
      "|    n_updates            | 30        |\n",
      "|    policy_gradient_loss | -0.00233  |\n",
      "|    std                  | 0.966     |\n",
      "|    value_loss           | 0.894     |\n",
      "---------------------------------------\n",
      "Num timesteps: 9000\n",
      "Best mean reward: -53.92 - Last mean reward per episode: -52.50\n",
      "Saving new best model to tmp/best_model.zip\n",
      "Num timesteps: 10000\n",
      "Best mean reward: -52.50 - Last mean reward per episode: -53.40\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -53.4        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 23           |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 433          |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0047936784 |\n",
      "|    clip_fraction        | 0.0377       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -4.14        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.411        |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.00292     |\n",
      "|    std                  | 0.959        |\n",
      "|    value_loss           | 0.947        |\n",
      "------------------------------------------\n",
      "Num timesteps: 11000\n",
      "Best mean reward: -52.50 - Last mean reward per episode: -51.01\n",
      "Saving new best model to tmp/best_model.zip\n",
      "Num timesteps: 12000\n",
      "Best mean reward: -51.01 - Last mean reward per episode: -49.54\n",
      "Saving new best model to tmp/best_model.zip\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -49.5       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 23          |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 534         |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008582542 |\n",
      "|    clip_fraction        | 0.103       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -4.1        |\n",
      "|    explained_variance   | 1.73e-06    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.472       |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.00874    |\n",
      "|    std                  | 0.944       |\n",
      "|    value_loss           | 0.973       |\n",
      "-----------------------------------------\n",
      "Num timesteps: 13000\n",
      "Best mean reward: -49.54 - Last mean reward per episode: -47.92\n",
      "Saving new best model to tmp/best_model.zip\n",
      "Num timesteps: 14000\n",
      "Best mean reward: -47.92 - Last mean reward per episode: -47.37\n",
      "Saving new best model to tmp/best_model.zip\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -47.4        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 22           |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 635          |\n",
      "|    total_timesteps      | 14336        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053116838 |\n",
      "|    clip_fraction        | 0.0553       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -4.07        |\n",
      "|    explained_variance   | 0.0072       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.393        |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | -0.00448     |\n",
      "|    std                  | 0.936        |\n",
      "|    value_loss           | 1.01         |\n",
      "------------------------------------------\n",
      "Num timesteps: 15000\n",
      "Best mean reward: -47.37 - Last mean reward per episode: -46.58\n",
      "Saving new best model to tmp/best_model.zip\n",
      "Num timesteps: 16000\n",
      "Best mean reward: -46.58 - Last mean reward per episode: -46.04\n",
      "Saving new best model to tmp/best_model.zip\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -46          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 22           |\n",
      "|    iterations           | 8            |\n",
      "|    time_elapsed         | 730          |\n",
      "|    total_timesteps      | 16384        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0058633694 |\n",
      "|    clip_fraction        | 0.0592       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -4.05        |\n",
      "|    explained_variance   | 0.00483      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.381        |\n",
      "|    n_updates            | 70           |\n",
      "|    policy_gradient_loss | -0.004       |\n",
      "|    std                  | 0.933        |\n",
      "|    value_loss           | 1.11         |\n",
      "------------------------------------------\n",
      "Num timesteps: 17000\n",
      "Best mean reward: -46.04 - Last mean reward per episode: -46.58\n",
      "Num timesteps: 18000\n",
      "Best mean reward: -46.04 - Last mean reward per episode: -46.10\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -46.1       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 22          |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 825         |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006197987 |\n",
      "|    clip_fraction        | 0.0664      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -4.04       |\n",
      "|    explained_variance   | 0.00627     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.696       |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.00505    |\n",
      "|    std                  | 0.93        |\n",
      "|    value_loss           | 1.47        |\n",
      "-----------------------------------------\n",
      "Num timesteps: 19000\n",
      "Best mean reward: -46.04 - Last mean reward per episode: -45.97\n",
      "Saving new best model to tmp/best_model.zip\n",
      "Num timesteps: 20000\n",
      "Best mean reward: -45.97 - Last mean reward per episode: -46.49\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -46.5       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 22          |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 922         |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006780885 |\n",
      "|    clip_fraction        | 0.057       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -4.02       |\n",
      "|    explained_variance   | 0.01        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.419       |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.00352    |\n",
      "|    std                  | 0.918       |\n",
      "|    value_loss           | 1.3         |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[39m# Train the agent and display a progress bar\u001b[39;00m\n\u001b[0;32m      4\u001b[0m timesteps \u001b[39m=\u001b[39m \u001b[39m90000\u001b[39m\n\u001b[1;32m----> 5\u001b[0m model\u001b[39m.\u001b[39;49mlearn(total_timesteps\u001b[39m=\u001b[39;49m\u001b[39mint\u001b[39;49m(timesteps), callback\u001b[39m=\u001b[39;49mcallback)\n",
      "File \u001b[1;32mc:\\Users\\candidomariz\\AppData\\Local\\miniconda3\\lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:308\u001b[0m, in \u001b[0;36mPPO.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    299\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlearn\u001b[39m(\n\u001b[0;32m    300\u001b[0m     \u001b[39mself\u001b[39m: SelfPPO,\n\u001b[0;32m    301\u001b[0m     total_timesteps: \u001b[39mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    306\u001b[0m     progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    307\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m SelfPPO:\n\u001b[1;32m--> 308\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mlearn(\n\u001b[0;32m    309\u001b[0m         total_timesteps\u001b[39m=\u001b[39;49mtotal_timesteps,\n\u001b[0;32m    310\u001b[0m         callback\u001b[39m=\u001b[39;49mcallback,\n\u001b[0;32m    311\u001b[0m         log_interval\u001b[39m=\u001b[39;49mlog_interval,\n\u001b[0;32m    312\u001b[0m         tb_log_name\u001b[39m=\u001b[39;49mtb_log_name,\n\u001b[0;32m    313\u001b[0m         reset_num_timesteps\u001b[39m=\u001b[39;49mreset_num_timesteps,\n\u001b[0;32m    314\u001b[0m         progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[0;32m    315\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\candidomariz\\AppData\\Local\\miniconda3\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:281\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    278\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlogger\u001b[39m.\u001b[39mrecord(\u001b[39m\"\u001b[39m\u001b[39mtime/total_timesteps\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps, exclude\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtensorboard\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    279\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlogger\u001b[39m.\u001b[39mdump(step\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps)\n\u001b[1;32m--> 281\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain()\n\u001b[0;32m    283\u001b[0m callback\u001b[39m.\u001b[39mon_training_end()\n\u001b[0;32m    285\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\candidomariz\\AppData\\Local\\miniconda3\\lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:272\u001b[0m, in \u001b[0;36mPPO.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    270\u001b[0m \u001b[39m# Optimization step\u001b[39;00m\n\u001b[0;32m    271\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpolicy\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m--> 272\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m    273\u001b[0m \u001b[39m# Clip grad norm\u001b[39;00m\n\u001b[0;32m    274\u001b[0m th\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mclip_grad_norm_(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpolicy\u001b[39m.\u001b[39mparameters(), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_grad_norm)\n",
      "File \u001b[1;32mc:\\Users\\candidomariz\\AppData\\Local\\miniconda3\\lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    489\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\candidomariz\\AppData\\Local\\miniconda3\\lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Instantiate the agent\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "# Train the agent and display a progress bar\n",
    "timesteps = 90000\n",
    "model.learn(total_timesteps=int(timesteps), callback=callback)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_reward, std_reward = evaluate_policy(model, model.get_env(), n_eval_episodes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-49.3209038 29.719247237129636\n"
     ]
    }
   ],
   "source": [
    "print(mean_reward, std_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index -1 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Helper from the library\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m results_plotter\u001b[39m.\u001b[39;49mplot_results(\n\u001b[0;32m      3\u001b[0m     [log_dir], timesteps, results_plotter\u001b[39m.\u001b[39;49mX_TIMESTEPS, \u001b[39m\"\u001b[39;49m\u001b[39mPPO CarRacing-v2\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[0;32m      4\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\candidomariz\\AppData\\Local\\miniconda3\\lib\\site-packages\\stable_baselines3\\common\\results_plotter.py:122\u001b[0m, in \u001b[0;36mplot_results\u001b[1;34m(dirs, num_timesteps, x_axis, task_name, figsize)\u001b[0m\n\u001b[0;32m    120\u001b[0m     data_frames\u001b[39m.\u001b[39mappend(data_frame)\n\u001b[0;32m    121\u001b[0m xy_list \u001b[39m=\u001b[39m [ts2xy(data_frame, x_axis) \u001b[39mfor\u001b[39;00m data_frame \u001b[39min\u001b[39;00m data_frames]\n\u001b[1;32m--> 122\u001b[0m plot_curves(xy_list, x_axis, task_name, figsize)\n",
      "File \u001b[1;32mc:\\Users\\candidomariz\\AppData\\Local\\miniconda3\\lib\\site-packages\\stable_baselines3\\common\\results_plotter.py:85\u001b[0m, in \u001b[0;36mplot_curves\u001b[1;34m(xy_list, x_axis, title, figsize)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \u001b[39mplot the curves\u001b[39;00m\n\u001b[0;32m     76\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[39m:param figsize: Size of the figure (width, height)\u001b[39;00m\n\u001b[0;32m     82\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     84\u001b[0m plt\u001b[39m.\u001b[39mfigure(title, figsize\u001b[39m=\u001b[39mfigsize)\n\u001b[1;32m---> 85\u001b[0m max_x \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39;49m(xy[\u001b[39m0\u001b[39;49m][\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m] \u001b[39mfor\u001b[39;49;00m xy \u001b[39min\u001b[39;49;00m xy_list)\n\u001b[0;32m     86\u001b[0m min_x \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m     87\u001b[0m \u001b[39mfor\u001b[39;00m _, (x, y) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(xy_list):\n",
      "File \u001b[1;32mc:\\Users\\candidomariz\\AppData\\Local\\miniconda3\\lib\\site-packages\\stable_baselines3\\common\\results_plotter.py:85\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \u001b[39mplot the curves\u001b[39;00m\n\u001b[0;32m     76\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[39m:param figsize: Size of the figure (width, height)\u001b[39;00m\n\u001b[0;32m     82\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     84\u001b[0m plt\u001b[39m.\u001b[39mfigure(title, figsize\u001b[39m=\u001b[39mfigsize)\n\u001b[1;32m---> 85\u001b[0m max_x \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(xy[\u001b[39m0\u001b[39;49m][\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m] \u001b[39mfor\u001b[39;00m xy \u001b[39min\u001b[39;00m xy_list)\n\u001b[0;32m     86\u001b[0m min_x \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m     87\u001b[0m \u001b[39mfor\u001b[39;00m _, (x, y) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(xy_list):\n",
      "\u001b[1;31mIndexError\u001b[0m: index -1 is out of bounds for axis 0 with size 0"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Helper from the library\n",
    "results_plotter.plot_results(\n",
    "    [log_dir], timesteps, results_plotter.X_TIMESTEPS, \"PPO CarRacing-v2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plot_results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m plot_results(log_dir)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plot_results' is not defined"
     ]
    }
   ],
   "source": [
    "plot_results(log_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    }
   ],
   "source": [
    "test_env = gym.make(\"CarRacing-v2\", render_mode='human')\n",
    "dir = os.path.join(log_dir, \"best_model.zip\")\n",
    "model = PPO.load(dir, env=test_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_env = model.get_env()\n",
    "obs = vec_env.reset()\n",
    "while True:\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, done, info = vec_env.step(action)\n",
    "    #obs, rewards, dones, info = test_env.step(action)\n",
    "    vec_env.render()\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "vec_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    }
   ],
   "source": [
    "test_env = gym.make(\"CarRacing-v2\", render_mode='rgb_array')\n",
    "dir = os.path.join(log_dir, \"best_model\")\n",
    "create_gif(test_env, dir, log_dir, \"ppo_car\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discrete space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment\n",
    "env = gym.make(\"CarRacing-v2\", continuous=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -58      |\n",
      "| time/              |          |\n",
      "|    fps             | 29       |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 69       |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the agent\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "# Train the agent and display a progress bar\n",
    "model.learn(total_timesteps=int(1000), callback=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_reward, std_reward = evaluate_policy(model, model.get_env(), n_eval_episodes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-35.794415900000004 45.31521641763898\n"
     ]
    }
   ],
   "source": [
    "print(mean_reward, std_reward)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    }
   ],
   "source": [
    "test_env = gym.make(\"CarRacing-v2\", render_mode='human', continuous=False)\n",
    "dir = os.path.join(log_dir, \"best_model.zip\")\n",
    "model = PPO.load(dir, env=test_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_env = model.get_env()\n",
    "obs = vec_env.reset()\n",
    "while True:\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, done, info = vec_env.step(action)\n",
    "    #obs, rewards, dones, info = test_env.step(action)\n",
    "    vec_env.render()\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "vec_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_env = gym.make(\"CarRacing-v2\", render_mode='rgb_array', contunous=False)\n",
    "dir = os.path.join(log_dir, \"best_model\")\n",
    "create_gif(test_env, dir, log_dir, \"ppo_car\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lap completion percentage change "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment\n",
    "env = gym.make(\"CarRacing-v2\", lap_complete_percentage=0.5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the agent\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "# Train the agent and display a progress bar\n",
    "model.learn(total_timesteps=int(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_reward, std_reward = evaluate_policy(model, model.get_env(), n_eval_episodes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mean_reward, std_reward)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_env = gym.make(\"CarRacing-v2\", render_mode='human', continuous=False)\n",
    "dir = os.path.join(log_dir, \"best_model.zip\")\n",
    "model = PPO.load(dir, env=test_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_env = model.get_env()\n",
    "obs = vec_env.reset()\n",
    "while True:\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, done, info = vec_env.step(action)\n",
    "    #obs, rewards, dones, info = test_env.step(action)\n",
    "    vec_env.render()\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "vec_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_env = gym.make(\"CarRacing-v2\", render_mode='rgb_array')\n",
    "dir = os.path.join(log_dir, \"best_model\")\n",
    "create_gif(test_env, dir, log_dir, \"ppo_car\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train with CNN instead of MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Policy MlpLstmPolicy unknown",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Instantiate the agent\u001b[39;00m\n\u001b[0;32m      2\u001b[0m env \u001b[39m=\u001b[39m gym\u001b[39m.\u001b[39mmake(\u001b[39m\"\u001b[39m\u001b[39mCarRacing-v2\u001b[39m\u001b[39m\"\u001b[39m, render_mode \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mrgb_array\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m model \u001b[39m=\u001b[39m PPO(\u001b[39m\"\u001b[39;49m\u001b[39mMlpLstmPolicy\u001b[39;49m\u001b[39m\"\u001b[39;49m, env, verbose\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[0;32m      4\u001b[0m \u001b[39m# Train the agent and display a progress bar\u001b[39;00m\n\u001b[0;32m      5\u001b[0m timesteps \u001b[39m=\u001b[39m \u001b[39m5000\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\candidomariz\\AppData\\Local\\miniconda3\\lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:104\u001b[0m, in \u001b[0;36mPPO.__init__\u001b[1;34m(self, policy, env, learning_rate, n_steps, batch_size, n_epochs, gamma, gae_lambda, clip_range, clip_range_vf, normalize_advantage, ent_coef, vf_coef, max_grad_norm, use_sde, sde_sample_freq, target_kl, stats_window_size, tensorboard_log, policy_kwargs, verbose, seed, device, _init_setup_model)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[0;32m     78\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m     79\u001b[0m     policy: Union[\u001b[39mstr\u001b[39m, Type[ActorCriticPolicy]],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    102\u001b[0m     _init_setup_model: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m    103\u001b[0m ):\n\u001b[1;32m--> 104\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[0;32m    105\u001b[0m         policy,\n\u001b[0;32m    106\u001b[0m         env,\n\u001b[0;32m    107\u001b[0m         learning_rate\u001b[39m=\u001b[39;49mlearning_rate,\n\u001b[0;32m    108\u001b[0m         n_steps\u001b[39m=\u001b[39;49mn_steps,\n\u001b[0;32m    109\u001b[0m         gamma\u001b[39m=\u001b[39;49mgamma,\n\u001b[0;32m    110\u001b[0m         gae_lambda\u001b[39m=\u001b[39;49mgae_lambda,\n\u001b[0;32m    111\u001b[0m         ent_coef\u001b[39m=\u001b[39;49ment_coef,\n\u001b[0;32m    112\u001b[0m         vf_coef\u001b[39m=\u001b[39;49mvf_coef,\n\u001b[0;32m    113\u001b[0m         max_grad_norm\u001b[39m=\u001b[39;49mmax_grad_norm,\n\u001b[0;32m    114\u001b[0m         use_sde\u001b[39m=\u001b[39;49muse_sde,\n\u001b[0;32m    115\u001b[0m         sde_sample_freq\u001b[39m=\u001b[39;49msde_sample_freq,\n\u001b[0;32m    116\u001b[0m         stats_window_size\u001b[39m=\u001b[39;49mstats_window_size,\n\u001b[0;32m    117\u001b[0m         tensorboard_log\u001b[39m=\u001b[39;49mtensorboard_log,\n\u001b[0;32m    118\u001b[0m         policy_kwargs\u001b[39m=\u001b[39;49mpolicy_kwargs,\n\u001b[0;32m    119\u001b[0m         verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[0;32m    120\u001b[0m         device\u001b[39m=\u001b[39;49mdevice,\n\u001b[0;32m    121\u001b[0m         seed\u001b[39m=\u001b[39;49mseed,\n\u001b[0;32m    122\u001b[0m         _init_setup_model\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    123\u001b[0m         supported_action_spaces\u001b[39m=\u001b[39;49m(\n\u001b[0;32m    124\u001b[0m             spaces\u001b[39m.\u001b[39;49mBox,\n\u001b[0;32m    125\u001b[0m             spaces\u001b[39m.\u001b[39;49mDiscrete,\n\u001b[0;32m    126\u001b[0m             spaces\u001b[39m.\u001b[39;49mMultiDiscrete,\n\u001b[0;32m    127\u001b[0m             spaces\u001b[39m.\u001b[39;49mMultiBinary,\n\u001b[0;32m    128\u001b[0m         ),\n\u001b[0;32m    129\u001b[0m     )\n\u001b[0;32m    131\u001b[0m     \u001b[39m# Sanity check, otherwise it will lead to noisy gradient and NaN\u001b[39;00m\n\u001b[0;32m    132\u001b[0m     \u001b[39m# because of the advantage normalization\u001b[39;00m\n\u001b[0;32m    133\u001b[0m     \u001b[39mif\u001b[39;00m normalize_advantage:\n",
      "File \u001b[1;32mc:\\Users\\candidomariz\\AppData\\Local\\miniconda3\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:81\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.__init__\u001b[1;34m(self, policy, env, learning_rate, n_steps, gamma, gae_lambda, ent_coef, vf_coef, max_grad_norm, use_sde, sde_sample_freq, stats_window_size, tensorboard_log, monitor_wrapper, policy_kwargs, verbose, seed, device, _init_setup_model, supported_action_spaces)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[0;32m     59\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m     60\u001b[0m     policy: Union[\u001b[39mstr\u001b[39m, Type[ActorCriticPolicy]],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     79\u001b[0m     supported_action_spaces: Optional[Tuple[Type[spaces\u001b[39m.\u001b[39mSpace], \u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m     80\u001b[0m ):\n\u001b[1;32m---> 81\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[0;32m     82\u001b[0m         policy\u001b[39m=\u001b[39;49mpolicy,\n\u001b[0;32m     83\u001b[0m         env\u001b[39m=\u001b[39;49menv,\n\u001b[0;32m     84\u001b[0m         learning_rate\u001b[39m=\u001b[39;49mlearning_rate,\n\u001b[0;32m     85\u001b[0m         policy_kwargs\u001b[39m=\u001b[39;49mpolicy_kwargs,\n\u001b[0;32m     86\u001b[0m         verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[0;32m     87\u001b[0m         device\u001b[39m=\u001b[39;49mdevice,\n\u001b[0;32m     88\u001b[0m         use_sde\u001b[39m=\u001b[39;49muse_sde,\n\u001b[0;32m     89\u001b[0m         sde_sample_freq\u001b[39m=\u001b[39;49msde_sample_freq,\n\u001b[0;32m     90\u001b[0m         support_multi_env\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m     91\u001b[0m         seed\u001b[39m=\u001b[39;49mseed,\n\u001b[0;32m     92\u001b[0m         stats_window_size\u001b[39m=\u001b[39;49mstats_window_size,\n\u001b[0;32m     93\u001b[0m         tensorboard_log\u001b[39m=\u001b[39;49mtensorboard_log,\n\u001b[0;32m     94\u001b[0m         supported_action_spaces\u001b[39m=\u001b[39;49msupported_action_spaces,\n\u001b[0;32m     95\u001b[0m     )\n\u001b[0;32m     97\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_steps \u001b[39m=\u001b[39m n_steps\n\u001b[0;32m     98\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgamma \u001b[39m=\u001b[39m gamma\n",
      "File \u001b[1;32mc:\\Users\\candidomariz\\AppData\\Local\\miniconda3\\lib\\site-packages\\stable_baselines3\\common\\base_class.py:123\u001b[0m, in \u001b[0;36mBaseAlgorithm.__init__\u001b[1;34m(self, policy, env, learning_rate, policy_kwargs, stats_window_size, tensorboard_log, verbose, device, support_multi_env, monitor_wrapper, seed, use_sde, sde_sample_freq, supported_action_spaces)\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[0;32m    106\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    107\u001b[0m     policy: Union[\u001b[39mstr\u001b[39m, Type[BasePolicy]],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    120\u001b[0m     supported_action_spaces: Optional[Tuple[Type[spaces\u001b[39m.\u001b[39mSpace], \u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    121\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    122\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(policy, \u001b[39mstr\u001b[39m):\n\u001b[1;32m--> 123\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpolicy_class \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_policy_from_name(policy)\n\u001b[0;32m    124\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    125\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpolicy_class \u001b[39m=\u001b[39m policy\n",
      "File \u001b[1;32mc:\\Users\\candidomariz\\AppData\\Local\\miniconda3\\lib\\site-packages\\stable_baselines3\\common\\base_class.py:338\u001b[0m, in \u001b[0;36mBaseAlgorithm._get_policy_from_name\u001b[1;34m(self, policy_name)\u001b[0m\n\u001b[0;32m    336\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpolicy_aliases[policy_name]\n\u001b[0;32m    337\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 338\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPolicy \u001b[39m\u001b[39m{\u001b[39;00mpolicy_name\u001b[39m}\u001b[39;00m\u001b[39m unknown\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Policy MlpLstmPolicy unknown"
     ]
    }
   ],
   "source": [
    "# Instantiate the agent\n",
    "env = gym.make(\"CarRacing-v2\", render_mode = \"rgb_array\")\n",
    "model = PPO(\"MlpLstmPolicy\", env, verbose=1)\n",
    "# Train the agent and display a progress bar\n",
    "timesteps = 5000\n",
    "model.learn(total_timesteps=int(timesteps), callback=callback)\n",
    "model.save(\"ppo_cnn\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-29.445576 25.004705077465335\n"
     ]
    }
   ],
   "source": [
    "mean_reward, std_reward = evaluate_policy(model, model.get_env(), n_eval_episodes=10)\n",
    "\n",
    "print(mean_reward, std_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    }
   ],
   "source": [
    "test_env = gym.make(\"CarRacing-v2\", render_mode='human')\n",
    "dir = os.path.join('', \"ppo_cnn.zip\")\n",
    "model = PPO.load(dir, env=test_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_env = model.get_env()\n",
    "obs = vec_env.reset()\n",
    "while True:\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, done, info = vec_env.step(action)\n",
    "    #obs, rewards, dones, info = test_env.step(action)\n",
    "    vec_env.render()\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "vec_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'tmp\\\\best_model'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m test_env \u001b[39m=\u001b[39m gym\u001b[39m.\u001b[39mmake(\u001b[39m\"\u001b[39m\u001b[39mCarRacing-v2\u001b[39m\u001b[39m\"\u001b[39m, render_mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mrgb_array\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      2\u001b[0m \u001b[39mdir\u001b[39m \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(log_dir, \u001b[39m\"\u001b[39m\u001b[39mbest_model\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m create_gif(test_env, \u001b[39mdir\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mppo_car\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "Cell \u001b[1;32mIn[4], line 6\u001b[0m, in \u001b[0;36mcreate_gif\u001b[1;34m(env, model_path, path, name)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate_gif\u001b[39m(env, model_path, path, name):\n\u001b[1;32m----> 6\u001b[0m     model \u001b[39m=\u001b[39m A2C\u001b[39m.\u001b[39;49mload(model_path, env\u001b[39m=\u001b[39;49menv)\n\u001b[0;32m      7\u001b[0m     images \u001b[39m=\u001b[39m []\n\u001b[0;32m      9\u001b[0m     vec_env \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mget_env()\n",
      "File \u001b[1;32mc:\\Users\\candidomariz\\AppData\\Local\\miniconda3\\lib\\site-packages\\stable_baselines3\\common\\base_class.py:679\u001b[0m, in \u001b[0;36mBaseAlgorithm.load\u001b[1;34m(cls, path, env, device, custom_objects, print_system_info, force_reset, **kwargs)\u001b[0m\n\u001b[0;32m    676\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m== CURRENT SYSTEM INFO ==\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    677\u001b[0m     get_system_info()\n\u001b[1;32m--> 679\u001b[0m data, params, pytorch_variables \u001b[39m=\u001b[39m load_from_zip_file(\n\u001b[0;32m    680\u001b[0m     path,\n\u001b[0;32m    681\u001b[0m     device\u001b[39m=\u001b[39;49mdevice,\n\u001b[0;32m    682\u001b[0m     custom_objects\u001b[39m=\u001b[39;49mcustom_objects,\n\u001b[0;32m    683\u001b[0m     print_system_info\u001b[39m=\u001b[39;49mprint_system_info,\n\u001b[0;32m    684\u001b[0m )\n\u001b[0;32m    686\u001b[0m \u001b[39massert\u001b[39;00m data \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mNo data found in the saved file\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    687\u001b[0m \u001b[39massert\u001b[39;00m params \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mNo params found in the saved file\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\candidomariz\\AppData\\Local\\miniconda3\\lib\\site-packages\\stable_baselines3\\common\\save_util.py:390\u001b[0m, in \u001b[0;36mload_from_zip_file\u001b[1;34m(load_path, load_data, custom_objects, device, verbose, print_system_info)\u001b[0m\n\u001b[0;32m    363\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_from_zip_file\u001b[39m(\n\u001b[0;32m    364\u001b[0m     load_path: Union[\u001b[39mstr\u001b[39m, pathlib\u001b[39m.\u001b[39mPath, io\u001b[39m.\u001b[39mBufferedIOBase],\n\u001b[0;32m    365\u001b[0m     load_data: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    369\u001b[0m     print_system_info: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    370\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Optional[Dict[\u001b[39mstr\u001b[39m, Any]], TensorDict, Optional[TensorDict]]:\n\u001b[0;32m    371\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    372\u001b[0m \u001b[39m    Load model data from a .zip archive\u001b[39;00m\n\u001b[0;32m    373\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    388\u001b[0m \u001b[39m        and dict of pytorch variables\u001b[39;00m\n\u001b[0;32m    389\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 390\u001b[0m     load_path \u001b[39m=\u001b[39m open_path(load_path, \u001b[39m\"\u001b[39;49m\u001b[39mr\u001b[39;49m\u001b[39m\"\u001b[39;49m, verbose\u001b[39m=\u001b[39;49mverbose, suffix\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mzip\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m    392\u001b[0m     \u001b[39m# set device to cpu if cuda is not available\u001b[39;00m\n\u001b[0;32m    393\u001b[0m     device \u001b[39m=\u001b[39m get_device(device\u001b[39m=\u001b[39mdevice)\n",
      "File \u001b[1;32mc:\\Users\\candidomariz\\AppData\\Local\\miniconda3\\lib\\functools.py:888\u001b[0m, in \u001b[0;36msingledispatch.<locals>.wrapper\u001b[1;34m(*args, **kw)\u001b[0m\n\u001b[0;32m    884\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m args:\n\u001b[0;32m    885\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mfuncname\u001b[39m}\u001b[39;00m\u001b[39m requires at least \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    886\u001b[0m                     \u001b[39m'\u001b[39m\u001b[39m1 positional argument\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m--> 888\u001b[0m \u001b[39mreturn\u001b[39;00m dispatch(args[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw)\n",
      "File \u001b[1;32mc:\\Users\\candidomariz\\AppData\\Local\\miniconda3\\lib\\site-packages\\stable_baselines3\\common\\save_util.py:234\u001b[0m, in \u001b[0;36mopen_path_str\u001b[1;34m(path, mode, verbose, suffix)\u001b[0m\n\u001b[0;32m    219\u001b[0m \u001b[39m@open_path\u001b[39m\u001b[39m.\u001b[39mregister(\u001b[39mstr\u001b[39m)\n\u001b[0;32m    220\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mopen_path_str\u001b[39m(path: \u001b[39mstr\u001b[39m, mode: \u001b[39mstr\u001b[39m, verbose: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m, suffix: Optional[\u001b[39mstr\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m io\u001b[39m.\u001b[39mBufferedIOBase:\n\u001b[0;32m    221\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    222\u001b[0m \u001b[39m    Open a path given by a string. If writing to the path, the function ensures\u001b[39;00m\n\u001b[0;32m    223\u001b[0m \u001b[39m    that the path exists.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[39m    :return:\u001b[39;00m\n\u001b[0;32m    233\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 234\u001b[0m     \u001b[39mreturn\u001b[39;00m open_path(pathlib\u001b[39m.\u001b[39;49mPath(path), mode, verbose, suffix)\n",
      "File \u001b[1;32mc:\\Users\\candidomariz\\AppData\\Local\\miniconda3\\lib\\functools.py:888\u001b[0m, in \u001b[0;36msingledispatch.<locals>.wrapper\u001b[1;34m(*args, **kw)\u001b[0m\n\u001b[0;32m    884\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m args:\n\u001b[0;32m    885\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mfuncname\u001b[39m}\u001b[39;00m\u001b[39m requires at least \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    886\u001b[0m                     \u001b[39m'\u001b[39m\u001b[39m1 positional argument\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m--> 888\u001b[0m \u001b[39mreturn\u001b[39;00m dispatch(args[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw)\n",
      "File \u001b[1;32mc:\\Users\\candidomariz\\AppData\\Local\\miniconda3\\lib\\site-packages\\stable_baselines3\\common\\save_util.py:258\u001b[0m, in \u001b[0;36mopen_path_pathlib\u001b[1;34m(path, mode, verbose, suffix)\u001b[0m\n\u001b[0;32m    256\u001b[0m \u001b[39mif\u001b[39;00m mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    257\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 258\u001b[0m         path \u001b[39m=\u001b[39m path\u001b[39m.\u001b[39;49mopen(\u001b[39m\"\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m    259\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m \u001b[39mas\u001b[39;00m error:\n\u001b[0;32m    260\u001b[0m         \u001b[39mif\u001b[39;00m suffix \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m suffix \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\candidomariz\\AppData\\Local\\miniconda3\\lib\\pathlib.py:1252\u001b[0m, in \u001b[0;36mPath.open\u001b[1;34m(self, mode, buffering, encoding, errors, newline)\u001b[0m\n\u001b[0;32m   1246\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39mself\u001b[39m, mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m, buffering\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, encoding\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   1247\u001b[0m          errors\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, newline\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m   1248\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1249\u001b[0m \u001b[39m    Open the file pointed by this path and return a file object, as\u001b[39;00m\n\u001b[0;32m   1250\u001b[0m \u001b[39m    the built-in open() function does.\u001b[39;00m\n\u001b[0;32m   1251\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1252\u001b[0m     \u001b[39mreturn\u001b[39;00m io\u001b[39m.\u001b[39;49mopen(\u001b[39mself\u001b[39;49m, mode, buffering, encoding, errors, newline,\n\u001b[0;32m   1253\u001b[0m                    opener\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_opener)\n",
      "File \u001b[1;32mc:\\Users\\candidomariz\\AppData\\Local\\miniconda3\\lib\\pathlib.py:1120\u001b[0m, in \u001b[0;36mPath._opener\u001b[1;34m(self, name, flags, mode)\u001b[0m\n\u001b[0;32m   1118\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_opener\u001b[39m(\u001b[39mself\u001b[39m, name, flags, mode\u001b[39m=\u001b[39m\u001b[39m0o666\u001b[39m):\n\u001b[0;32m   1119\u001b[0m     \u001b[39m# A stub for the opener argument to built-in open()\u001b[39;00m\n\u001b[1;32m-> 1120\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_accessor\u001b[39m.\u001b[39;49mopen(\u001b[39mself\u001b[39;49m, flags, mode)\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'tmp\\\\best_model'"
     ]
    }
   ],
   "source": [
    "test_env = gym.make(\"CarRacing-v2\", render_mode='rgb_array')\n",
    "dir = os.path.join(log_dir, \"best_model\")\n",
    "create_gif(test_env, dir, '', \"ppo_car\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gymnasium as gym\n",
    "\n",
    "from stable_baselines3 import A2C\n",
    "from stable_baselines3.common import results_plotter\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.results_plotter import load_results, ts2xy\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveOnBestTrainingRewardCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    Callback for saving a model (the check is done every ``check_freq`` steps)\n",
    "    based on the training reward (in practice, we recommend using ``EvalCallback``).\n",
    "\n",
    "    :param check_freq: (int)\n",
    "    :param log_dir: (str) Path to the folder where the model will be saved.\n",
    "      It must contains the file created by the ``Monitor`` wrapper.\n",
    "    :param verbose: (int)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, check_freq: int, log_dir: str, verbose=1):\n",
    "        super().__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.log_dir = log_dir\n",
    "        self.save_path = os.path.join(log_dir, \"best_model\")\n",
    "        self.best_mean_reward = -np.inf\n",
    "\n",
    "    # def _init_callback(self) -> None:\n",
    "    #     # Create folder if needed\n",
    "    #     if self.save_path is not None:\n",
    "    #         os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "\n",
    "            # Retrieve training reward\n",
    "            x, y = ts2xy(load_results(self.log_dir), \"timesteps\")\n",
    "            if len(x) > 0:\n",
    "                # Mean training reward over the last 100 episodes\n",
    "                mean_reward = np.mean(y[-100:])\n",
    "                if self.verbose > 0:\n",
    "                    print(f\"Num timesteps: {self.num_timesteps}\")\n",
    "                    print(\n",
    "                        f\"Best mean reward: {self.best_mean_reward:.2f} - Last mean reward per episode: {mean_reward:.2f}\"\n",
    "                    )\n",
    "\n",
    "                # New best model, you could save the agent here\n",
    "                if mean_reward > self.best_mean_reward:\n",
    "                    self.best_mean_reward = mean_reward\n",
    "                    # Example for saving best model\n",
    "                    if self.verbose > 0:\n",
    "                        print(f\"Saving new best model to {self.save_path}.zip\")\n",
    "                    self.model.save(self.save_path)\n",
    "\n",
    "        return True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(values, window):\n",
    "    \"\"\"\n",
    "    Smooth values by doing a moving average\n",
    "    :param values: (numpy array)\n",
    "    :param window: (int)\n",
    "    :return: (numpy array)\n",
    "    \"\"\"\n",
    "    weights = np.repeat(1.0, window) / window\n",
    "    return np.convolve(values, weights, \"valid\")\n",
    "\n",
    "\n",
    "def plot_results(log_folder, title=\"Learning Curve\"):\n",
    "    \"\"\"\n",
    "    plot the results\n",
    "\n",
    "    :param log_folder: (str) the save location of the results to plot\n",
    "    :param title: (str) the title of the task to plot\n",
    "    \"\"\"\n",
    "    x, y = ts2xy(load_results(log_folder), \"timesteps\")\n",
    "    #y = moving_average(y, window=50)\n",
    "    # Truncate x\n",
    "    #x = x[len(x) - len(y) :]\n",
    "\n",
    "    fig = plt.figure(title)\n",
    "    plt.plot(x, y)\n",
    "    plt.xlabel(\"Number of Timesteps\")\n",
    "    plt.ylabel(\"Rewards\")\n",
    "    plt.title(title + \" Smoothed\")\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Gif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "import numpy as np\n",
    "from stable_baselines3 import A2C\n",
    "\n",
    "def create_gif(env, model_path, path, name):\n",
    "    model = A2C.load(model_path, env=env)\n",
    "    images = []\n",
    "\n",
    "    vec_env = model.get_env()\n",
    "    obs = vec_env.reset()\n",
    "    img = vec_env.render()\n",
    "    for i in range(500):\n",
    "        images.append(img)\n",
    "        action, _ = model.predict(obs)\n",
    "        obs, _, _ ,_ = vec_env.step(action)\n",
    "        img = vec_env.render(mode='rgb_array')\n",
    "    gif_name =  path + name + '.gif'\n",
    "    imageio.mimsave(gif_name, [np.array(img) for i, img in enumerate(images) if i%2 == 0], duration=120)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment\n",
    "env = gym.make(\"CarRacing-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup callback\n",
    "\n",
    "# Create log dir\n",
    "log_dir = \"a2c_normal/\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# Wrap the environment\n",
    "env = Monitor(env, log_dir)\n",
    "\n",
    "callback = SaveOnBestTrainingRewardCallback(check_freq=1000, log_dir=log_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 30       |\n",
      "|    iterations         | 100      |\n",
      "|    time_elapsed       | 16       |\n",
      "|    total_timesteps    | 500      |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -4.25    |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 99       |\n",
      "|    policy_loss        | 2.73     |\n",
      "|    std                | 0.997    |\n",
      "|    value_loss         | 2.36     |\n",
      "------------------------------------\n",
      "Num timesteps: 1000\n",
      "Best mean reward: -inf - Last mean reward per episode: -72.51\n",
      "Saving new best model to a2c_normal/best_model.zip\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1e+03    |\n",
      "|    ep_rew_mean        | -72.5    |\n",
      "| time/                 |          |\n",
      "|    fps                | 30       |\n",
      "|    iterations         | 200      |\n",
      "|    time_elapsed       | 32       |\n",
      "|    total_timesteps    | 1000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -4.25    |\n",
      "|    explained_variance | nan      |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 199      |\n",
      "|    policy_loss        | 1.75e-06 |\n",
      "|    std                | 0.997    |\n",
      "|    value_loss         | 0        |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1e+03    |\n",
      "|    ep_rew_mean        | -72.5    |\n",
      "| time/                 |          |\n",
      "|    fps                | 28       |\n",
      "|    iterations         | 300      |\n",
      "|    time_elapsed       | 52       |\n",
      "|    total_timesteps    | 1500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -4.24    |\n",
      "|    explained_variance | 1.97e-06 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 299      |\n",
      "|    policy_loss        | -0.0021  |\n",
      "|    std                | 0.994    |\n",
      "|    value_loss         | 4.19e-07 |\n",
      "------------------------------------\n",
      "Num timesteps: 2000\n",
      "Best mean reward: -72.51 - Last mean reward per episode: -69.77\n",
      "Saving new best model to a2c_normal/best_model.zip\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1e+03    |\n",
      "|    ep_rew_mean        | -69.8    |\n",
      "| time/                 |          |\n",
      "|    fps                | 27       |\n",
      "|    iterations         | 400      |\n",
      "|    time_elapsed       | 73       |\n",
      "|    total_timesteps    | 2000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -4.24    |\n",
      "|    explained_variance | nan      |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 399      |\n",
      "|    policy_loss        | 2.18e-06 |\n",
      "|    std                | 0.994    |\n",
      "|    value_loss         | 0        |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1e+03    |\n",
      "|    ep_rew_mean        | -69.8    |\n",
      "| time/                 |          |\n",
      "|    fps                | 27       |\n",
      "|    iterations         | 500      |\n",
      "|    time_elapsed       | 92       |\n",
      "|    total_timesteps    | 2500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -4.24    |\n",
      "|    explained_variance | nan      |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 499      |\n",
      "|    policy_loss        | 1.33e-06 |\n",
      "|    std                | 0.996    |\n",
      "|    value_loss         | 0        |\n",
      "------------------------------------\n",
      "Num timesteps: 3000\n",
      "Best mean reward: -69.77 - Last mean reward per episode: -72.73\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1e+03    |\n",
      "|    ep_rew_mean        | -72.7    |\n",
      "| time/                 |          |\n",
      "|    fps                | 26       |\n",
      "|    iterations         | 600      |\n",
      "|    time_elapsed       | 111      |\n",
      "|    total_timesteps    | 3000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -4.24    |\n",
      "|    explained_variance | nan      |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 599      |\n",
      "|    policy_loss        | 1.49e-06 |\n",
      "|    std                | 0.995    |\n",
      "|    value_loss         | 0        |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1e+03    |\n",
      "|    ep_rew_mean        | -72.7    |\n",
      "| time/                 |          |\n",
      "|    fps                | 26       |\n",
      "|    iterations         | 700      |\n",
      "|    time_elapsed       | 133      |\n",
      "|    total_timesteps    | 3500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -4.23    |\n",
      "|    explained_variance | nan      |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 699      |\n",
      "|    policy_loss        | 1.59e-06 |\n",
      "|    std                | 0.989    |\n",
      "|    value_loss         | 0        |\n",
      "------------------------------------\n",
      "Num timesteps: 4000\n",
      "Best mean reward: -69.77 - Last mean reward per episode: -73.14\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1e+03    |\n",
      "|    ep_rew_mean        | -73.1    |\n",
      "| time/                 |          |\n",
      "|    fps                | 26       |\n",
      "|    iterations         | 800      |\n",
      "|    time_elapsed       | 148      |\n",
      "|    total_timesteps    | 4000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -4.21    |\n",
      "|    explained_variance | nan      |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 799      |\n",
      "|    policy_loss        | 1.45e-06 |\n",
      "|    std                | 0.986    |\n",
      "|    value_loss         | 0        |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1e+03    |\n",
      "|    ep_rew_mean        | -73.1    |\n",
      "| time/                 |          |\n",
      "|    fps                | 26       |\n",
      "|    iterations         | 900      |\n",
      "|    time_elapsed       | 166      |\n",
      "|    total_timesteps    | 4500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -4.2     |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 899      |\n",
      "|    policy_loss        | -0.0202  |\n",
      "|    std                | 0.982    |\n",
      "|    value_loss         | 3.53e-05 |\n",
      "------------------------------------\n",
      "Num timesteps: 5000\n",
      "Best mean reward: -69.77 - Last mean reward per episode: -71.68\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1e+03    |\n",
      "|    ep_rew_mean        | -71.7    |\n",
      "| time/                 |          |\n",
      "|    fps                | 26       |\n",
      "|    iterations         | 1000     |\n",
      "|    time_elapsed       | 186      |\n",
      "|    total_timesteps    | 5000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -4.2     |\n",
      "|    explained_variance | nan      |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 999      |\n",
      "|    policy_loss        | 3.08e-06 |\n",
      "|    std                | 0.982    |\n",
      "|    value_loss         | 7.28e-13 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1e+03     |\n",
      "|    ep_rew_mean        | -71.7     |\n",
      "| time/                 |           |\n",
      "|    fps                | 26        |\n",
      "|    iterations         | 1100      |\n",
      "|    time_elapsed       | 207       |\n",
      "|    total_timesteps    | 5500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -4.21     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1099      |\n",
      "|    policy_loss        | -0.000167 |\n",
      "|    std                | 0.986     |\n",
      "|    value_loss         | 1.87e-09  |\n",
      "-------------------------------------\n",
      "Num timesteps: 6000\n",
      "Best mean reward: -69.77 - Last mean reward per episode: -71.64\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1e+03    |\n",
      "|    ep_rew_mean        | -71.6    |\n",
      "| time/                 |          |\n",
      "|    fps                | 26       |\n",
      "|    iterations         | 1200     |\n",
      "|    time_elapsed       | 227      |\n",
      "|    total_timesteps    | 6000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -4.21    |\n",
      "|    explained_variance | 0.0108   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1199     |\n",
      "|    policy_loss        | -3.8e-05 |\n",
      "|    std                | 0.983    |\n",
      "|    value_loss         | 8.06e-11 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1e+03     |\n",
      "|    ep_rew_mean        | -71.6     |\n",
      "| time/                 |           |\n",
      "|    fps                | 26        |\n",
      "|    iterations         | 1300      |\n",
      "|    time_elapsed       | 247       |\n",
      "|    total_timesteps    | 6500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -4.19     |\n",
      "|    explained_variance | 0.0581    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1299      |\n",
      "|    policy_loss        | -2.73e-05 |\n",
      "|    std                | 0.979     |\n",
      "|    value_loss         | 3.38e-11  |\n",
      "-------------------------------------\n",
      "Num timesteps: 7000\n",
      "Best mean reward: -69.77 - Last mean reward per episode: -71.02\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1e+03    |\n",
      "|    ep_rew_mean        | -71      |\n",
      "| time/                 |          |\n",
      "|    fps                | 25       |\n",
      "|    iterations         | 1400     |\n",
      "|    time_elapsed       | 273      |\n",
      "|    total_timesteps    | 7000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -4.19    |\n",
      "|    explained_variance | nan      |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1399     |\n",
      "|    policy_loss        | 1.5e-06  |\n",
      "|    std                | 0.978    |\n",
      "|    value_loss         | 0        |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1e+03     |\n",
      "|    ep_rew_mean        | -71       |\n",
      "| time/                 |           |\n",
      "|    fps                | 23        |\n",
      "|    iterations         | 1500      |\n",
      "|    time_elapsed       | 314       |\n",
      "|    total_timesteps    | 7500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -4.19     |\n",
      "|    explained_variance | 0.0581    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1499      |\n",
      "|    policy_loss        | -1.93e-05 |\n",
      "|    std                | 0.978     |\n",
      "|    value_loss         | 3.38e-11  |\n",
      "-------------------------------------\n",
      "Num timesteps: 8000\n",
      "Best mean reward: -69.77 - Last mean reward per episode: -71.83\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1e+03    |\n",
      "|    ep_rew_mean        | -71.8    |\n",
      "| time/                 |          |\n",
      "|    fps                | 23       |\n",
      "|    iterations         | 1600     |\n",
      "|    time_elapsed       | 339      |\n",
      "|    total_timesteps    | 8000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -4.19    |\n",
      "|    explained_variance | nan      |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1599     |\n",
      "|    policy_loss        | 1.41e-06 |\n",
      "|    std                | 0.977    |\n",
      "|    value_loss         | 0        |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1e+03    |\n",
      "|    ep_rew_mean        | -71.8    |\n",
      "| time/                 |          |\n",
      "|    fps                | 23       |\n",
      "|    iterations         | 1700     |\n",
      "|    time_elapsed       | 358      |\n",
      "|    total_timesteps    | 8500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -4.18    |\n",
      "|    explained_variance | nan      |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1699     |\n",
      "|    policy_loss        | 1.43e-06 |\n",
      "|    std                | 0.976    |\n",
      "|    value_loss         | 0        |\n",
      "------------------------------------\n",
      "Num timesteps: 9000\n",
      "Best mean reward: -69.77 - Last mean reward per episode: -71.65\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1e+03    |\n",
      "|    ep_rew_mean        | -71.7    |\n",
      "| time/                 |          |\n",
      "|    fps                | 23       |\n",
      "|    iterations         | 1800     |\n",
      "|    time_elapsed       | 378      |\n",
      "|    total_timesteps    | 9000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -4.18    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1799     |\n",
      "|    policy_loss        | -0.0291  |\n",
      "|    std                | 0.976    |\n",
      "|    value_loss         | 7.17e-05 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1e+03     |\n",
      "|    ep_rew_mean        | -71.7     |\n",
      "| time/                 |           |\n",
      "|    fps                | 23        |\n",
      "|    iterations         | 1900      |\n",
      "|    time_elapsed       | 396       |\n",
      "|    total_timesteps    | 9500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -4.19     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1899      |\n",
      "|    policy_loss        | -1.31e-05 |\n",
      "|    std                | 0.976     |\n",
      "|    value_loss         | 1e-11     |\n",
      "-------------------------------------\n",
      "Num timesteps: 10000\n",
      "Best mean reward: -69.77 - Last mean reward per episode: -72.06\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1e+03    |\n",
      "|    ep_rew_mean        | -72.1    |\n",
      "| time/                 |          |\n",
      "|    fps                | 24       |\n",
      "|    iterations         | 2000     |\n",
      "|    time_elapsed       | 414      |\n",
      "|    total_timesteps    | 10000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -4.19    |\n",
      "|    explained_variance | nan      |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1999     |\n",
      "|    policy_loss        | 1.49e-06 |\n",
      "|    std                | 0.976    |\n",
      "|    value_loss         | 0        |\n",
      "------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.a2c.a2c.A2C at 0x1e821b95f40>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate the agent\n",
    "#model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "model = A2C(\"MlpPolicy\", env, verbose=1)\n",
    "# Train the agent and display a progress bar\n",
    "timesteps = 10000\n",
    "model.learn(total_timesteps=int(timesteps), callback=callback)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_reward, std_reward = evaluate_policy(model, model.get_env(), n_eval_episodes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-93.3097544 0.5029659276339142\n"
     ]
    }
   ],
   "source": [
    "print(mean_reward, std_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper from the library\n",
    "#results_plotter.plot_results(\n",
    " #   [log_dir], timesteps, results_plotter.X_TIMESTEPS, \"A2C CarRacing-v2\"\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_results(log_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    }
   ],
   "source": [
    "test_env = gym.make(\"CarRacing-v2\", render_mode='human')\n",
    "dir = os.path.join(log_dir, \"best_model.zip\")\n",
    "model = A2C.load(dir, env=test_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_env = model.get_env()\n",
    "obs = vec_env.reset()\n",
    "while True:\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, done, info = vec_env.step(action)\n",
    "    #obs, rewards, dones, info = test_env.step(action)\n",
    "    vec_env.render()\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "vec_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    }
   ],
   "source": [
    "test_env = gym.make(\"CarRacing-v2\", render_mode='rgb_array')\n",
    "dir = os.path.join(log_dir, \"best_model\")\n",
    "create_gif(test_env, dir, log_dir, \"a2c_car\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discrete space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment\n",
    "env = gym.make(\"CarRacing-v2\", continuous=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup callback\n",
    "\n",
    "# Create log dir\n",
    "log_dir = \"a2c_discrete/\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# Wrap the environment\n",
    "env = Monitor(env, log_dir)\n",
    "\n",
    "callback = SaveOnBestTrainingRewardCallback(check_freq=1000, log_dir=log_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 30       |\n",
      "|    iterations         | 100      |\n",
      "|    time_elapsed       | 16       |\n",
      "|    total_timesteps    | 500      |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.37    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 99       |\n",
      "|    policy_loss        | -0.00878 |\n",
      "|    value_loss         | 4.76e-05 |\n",
      "------------------------------------\n",
      "Num timesteps: 1000\n",
      "Best mean reward: -62.83 - Last mean reward per episode: -85.89\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1e+03     |\n",
      "|    ep_rew_mean        | -72.9     |\n",
      "| time/                 |           |\n",
      "|    fps                | 26        |\n",
      "|    iterations         | 200       |\n",
      "|    time_elapsed       | 38        |\n",
      "|    total_timesteps    | 1000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.37     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 199       |\n",
      "|    policy_loss        | -2.47e-06 |\n",
      "|    value_loss         | 1e-11     |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1e+03    |\n",
      "|    ep_rew_mean        | -72.9    |\n",
      "| time/                 |          |\n",
      "|    fps                | 22       |\n",
      "|    iterations         | 300      |\n",
      "|    time_elapsed       | 66       |\n",
      "|    total_timesteps    | 1500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.13    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 299      |\n",
      "|    policy_loss        | -0.00194 |\n",
      "|    value_loss         | 7.79e-06 |\n",
      "------------------------------------\n",
      "Num timesteps: 2000\n",
      "Best mean reward: -62.83 - Last mean reward per episode: -84.48\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1e+03     |\n",
      "|    ep_rew_mean        | -73.7     |\n",
      "| time/                 |           |\n",
      "|    fps                | 22        |\n",
      "|    iterations         | 400       |\n",
      "|    time_elapsed       | 88        |\n",
      "|    total_timesteps    | 2000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.19     |\n",
      "|    explained_variance | 0.0058    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 399       |\n",
      "|    policy_loss        | -9.11e-06 |\n",
      "|    value_loss         | 1.43e-10  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1e+03    |\n",
      "|    ep_rew_mean        | -73.7    |\n",
      "| time/                 |          |\n",
      "|    fps                | 22       |\n",
      "|    iterations         | 500      |\n",
      "|    time_elapsed       | 111      |\n",
      "|    total_timesteps    | 2500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.31    |\n",
      "|    explained_variance | 4.77e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 499      |\n",
      "|    policy_loss        | -0.00364 |\n",
      "|    value_loss         | 1.07e-05 |\n",
      "------------------------------------\n",
      "Num timesteps: 3000\n",
      "Best mean reward: -62.83 - Last mean reward per episode: -84.01\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1e+03    |\n",
      "|    ep_rew_mean        | -75.9    |\n",
      "| time/                 |          |\n",
      "|    fps                | 22       |\n",
      "|    iterations         | 600      |\n",
      "|    time_elapsed       | 131      |\n",
      "|    total_timesteps    | 3000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.41    |\n",
      "|    explained_variance | nan      |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 599      |\n",
      "|    policy_loss        | 6.74e-07 |\n",
      "|    value_loss         | 0        |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1e+03     |\n",
      "|    ep_rew_mean        | -75.9     |\n",
      "| time/                 |           |\n",
      "|    fps                | 22        |\n",
      "|    iterations         | 700       |\n",
      "|    time_elapsed       | 154       |\n",
      "|    total_timesteps    | 3500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.41     |\n",
      "|    explained_variance | 0.0108    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 699       |\n",
      "|    policy_loss        | -8.65e-06 |\n",
      "|    value_loss         | 8.06e-11  |\n",
      "-------------------------------------\n",
      "Num timesteps: 4000\n",
      "Best mean reward: -62.83 - Last mean reward per episode: -82.92\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1e+03     |\n",
      "|    ep_rew_mean        | -75.2     |\n",
      "| time/                 |           |\n",
      "|    fps                | 22        |\n",
      "|    iterations         | 800       |\n",
      "|    time_elapsed       | 176       |\n",
      "|    total_timesteps    | 4000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.29     |\n",
      "|    explained_variance | 0.0055    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 799       |\n",
      "|    policy_loss        | -1.58e-05 |\n",
      "|    value_loss         | 3.35e-10  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1e+03    |\n",
      "|    ep_rew_mean        | -75.2    |\n",
      "| time/                 |          |\n",
      "|    fps                | 22       |\n",
      "|    iterations         | 900      |\n",
      "|    time_elapsed       | 198      |\n",
      "|    total_timesteps    | 4500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.958   |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 899      |\n",
      "|    policy_loss        | -0.00434 |\n",
      "|    value_loss         | 4.33e-05 |\n",
      "------------------------------------\n",
      "Num timesteps: 5000\n",
      "Best mean reward: -62.83 - Last mean reward per episode: -81.57\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1e+03    |\n",
      "|    ep_rew_mean        | -73.8    |\n",
      "| time/                 |          |\n",
      "|    fps                | 22       |\n",
      "|    iterations         | 1000     |\n",
      "|    time_elapsed       | 219      |\n",
      "|    total_timesteps    | 5000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.942   |\n",
      "|    explained_variance | nan      |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 999      |\n",
      "|    policy_loss        | 6.64e-07 |\n",
      "|    value_loss         | 0        |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1e+03    |\n",
      "|    ep_rew_mean        | -73.8    |\n",
      "| time/                 |          |\n",
      "|    fps                | 22       |\n",
      "|    iterations         | 1100     |\n",
      "|    time_elapsed       | 239      |\n",
      "|    total_timesteps    | 5500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.33    |\n",
      "|    explained_variance | 2.98e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1099     |\n",
      "|    policy_loss        | -0.00513 |\n",
      "|    value_loss         | 1.58e-05 |\n",
      "------------------------------------\n",
      "Num timesteps: 6000\n",
      "Best mean reward: -62.83 - Last mean reward per episode: -79.75\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1e+03     |\n",
      "|    ep_rew_mean        | -71.4     |\n",
      "| time/                 |           |\n",
      "|    fps                | 22        |\n",
      "|    iterations         | 1200      |\n",
      "|    time_elapsed       | 262       |\n",
      "|    total_timesteps    | 6000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.28     |\n",
      "|    explained_variance | 9.89e-06  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1199      |\n",
      "|    policy_loss        | -0.000225 |\n",
      "|    value_loss         | 2.04e-08  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1e+03     |\n",
      "|    ep_rew_mean        | -71.4     |\n",
      "| time/                 |           |\n",
      "|    fps                | 23        |\n",
      "|    iterations         | 1300      |\n",
      "|    time_elapsed       | 281       |\n",
      "|    total_timesteps    | 6500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.11     |\n",
      "|    explained_variance | 0.0036    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1299      |\n",
      "|    policy_loss        | -2.02e-05 |\n",
      "|    value_loss         | 2.29e-10  |\n",
      "-------------------------------------\n",
      "Num timesteps: 7000\n",
      "Best mean reward: -62.83 - Last mean reward per episode: -79.14\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1e+03     |\n",
      "|    ep_rew_mean        | -71.5     |\n",
      "| time/                 |           |\n",
      "|    fps                | 23        |\n",
      "|    iterations         | 1400      |\n",
      "|    time_elapsed       | 303       |\n",
      "|    total_timesteps    | 7000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.21     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1399      |\n",
      "|    policy_loss        | -0.00559  |\n",
      "|    value_loss         | 3.58e-05  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1e+03    |\n",
      "|    ep_rew_mean        | -71.5    |\n",
      "| time/                 |          |\n",
      "|    fps                | 23       |\n",
      "|    iterations         | 1500     |\n",
      "|    time_elapsed       | 321      |\n",
      "|    total_timesteps    | 7500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.33    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1499     |\n",
      "|    policy_loss        | -0.00437 |\n",
      "|    value_loss         | 1.24e-05 |\n",
      "------------------------------------\n",
      "Num timesteps: 8000\n",
      "Best mean reward: -62.83 - Last mean reward per episode: -76.89\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1e+03     |\n",
      "|    ep_rew_mean        | -68.5     |\n",
      "| time/                 |           |\n",
      "|    fps                | 23        |\n",
      "|    iterations         | 1600      |\n",
      "|    time_elapsed       | 341       |\n",
      "|    total_timesteps    | 8000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.38     |\n",
      "|    explained_variance | 0.00237   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1599      |\n",
      "|    policy_loss        | -2.44e-05 |\n",
      "|    value_loss         | 7.72e-10  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1e+03    |\n",
      "|    ep_rew_mean        | -68.5    |\n",
      "| time/                 |          |\n",
      "|    fps                | 23       |\n",
      "|    iterations         | 1700     |\n",
      "|    time_elapsed       | 363      |\n",
      "|    total_timesteps    | 8500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.35    |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1699     |\n",
      "|    policy_loss        | -0.00197 |\n",
      "|    value_loss         | 2.58e-06 |\n",
      "------------------------------------\n",
      "Num timesteps: 9000\n",
      "Best mean reward: -62.83 - Last mean reward per episode: -71.56\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1e+03    |\n",
      "|    ep_rew_mean        | -60.6    |\n",
      "| time/                 |          |\n",
      "|    fps                | 23       |\n",
      "|    iterations         | 1800     |\n",
      "|    time_elapsed       | 383      |\n",
      "|    total_timesteps    | 9000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.38    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1799     |\n",
      "|    policy_loss        | -0.00699 |\n",
      "|    value_loss         | 3.41e-05 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1e+03    |\n",
      "|    ep_rew_mean        | -60.6    |\n",
      "| time/                 |          |\n",
      "|    fps                | 23       |\n",
      "|    iterations         | 1900     |\n",
      "|    time_elapsed       | 406      |\n",
      "|    total_timesteps    | 9500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.28    |\n",
      "|    explained_variance | 1.79e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1899     |\n",
      "|    policy_loss        | -0.00143 |\n",
      "|    value_loss         | 1.01e-06 |\n",
      "------------------------------------\n",
      "Num timesteps: 10000\n",
      "Best mean reward: -62.83 - Last mean reward per episode: -70.22\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1e+03    |\n",
      "|    ep_rew_mean        | -59.5    |\n",
      "| time/                 |          |\n",
      "|    fps                | 23       |\n",
      "|    iterations         | 2000     |\n",
      "|    time_elapsed       | 424      |\n",
      "|    total_timesteps    | 10000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.28    |\n",
      "|    explained_variance | nan      |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1999     |\n",
      "|    policy_loss        | 5.13e-07 |\n",
      "|    value_loss         | 0        |\n",
      "------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.a2c.a2c.A2C at 0x1e8241f3c70>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate the agent\n",
    "model = A2C(\"MlpPolicy\", env, verbose=1)\n",
    "# Train the agent and display a progress bar\n",
    "timesteps = 10000\n",
    "model.learn(total_timesteps=int(timesteps), callback=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_reward, std_reward = evaluate_policy(model, model.get_env(), n_eval_episodes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-93.26666739999999 0.2900716381562324\n"
     ]
    }
   ],
   "source": [
    "print(mean_reward, std_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper from the library\n",
    "#results_plotter.plot_results(\n",
    " #   [log_dir], timesteps, results_plotter.X_TIMESTEPS, \"A2C CarRacing-v2\"\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_results(log_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    }
   ],
   "source": [
    "test_env = gym.make(\"CarRacing-v2\", render_mode='human', continuous=False)\n",
    "dir = os.path.join(log_dir, \"best_model.zip\")\n",
    "model = A2C.load(dir, env=test_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_env = model.get_env()\n",
    "obs = vec_env.reset()\n",
    "while True:\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, done, info = vec_env.step(action)\n",
    "    #obs, rewards, dones, info = test_env.step(action)\n",
    "    vec_env.render()\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "vec_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    }
   ],
   "source": [
    "test_env = gym.make(\"CarRacing-v2\", render_mode='rgb_array', continuous=False)\n",
    "dir = os.path.join(log_dir, \"best_model\")\n",
    "create_gif(test_env, dir, log_dir, \"a2c_car\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lap completion percentage change "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment\n",
    "env = gym.make(\"CarRacing-v2\", lap_complete_percent=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup callback\n",
    "\n",
    "# Create log dir\n",
    "log_dir = \"_lap/\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# Wrap the environment\n",
    "env = Monitor(env, log_dir)\n",
    "\n",
    "callback = SaveOnBestTrainingRewardCallback(check_freq=1000, log_dir=log_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 32       |\n",
      "|    iterations         | 100      |\n",
      "|    time_elapsed       | 15       |\n",
      "|    total_timesteps    | 500      |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -4.27    |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 99       |\n",
      "|    policy_loss        | -0.00624 |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 3.21e-06 |\n",
      "------------------------------------\n",
      "Num timesteps: 1000\n",
      "Best mean reward: -inf - Last mean reward per episode: -77.58\n",
      "Saving new best model to _lap/best_model.zip\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1e+03    |\n",
      "|    ep_rew_mean        | -67.6    |\n",
      "| time/                 |          |\n",
      "|    fps                | 28       |\n",
      "|    iterations         | 200      |\n",
      "|    time_elapsed       | 34       |\n",
      "|    total_timesteps    | 1000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -4.25    |\n",
      "|    explained_variance | 1.79e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 199      |\n",
      "|    policy_loss        | -0.0054  |\n",
      "|    std                | 0.998    |\n",
      "|    value_loss         | 1.65e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1e+03    |\n",
      "|    ep_rew_mean        | -67.6    |\n",
      "| time/                 |          |\n",
      "|    fps                | 27       |\n",
      "|    iterations         | 300      |\n",
      "|    time_elapsed       | 54       |\n",
      "|    total_timesteps    | 1500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -4.25    |\n",
      "|    explained_variance | 5.96e-08 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 299      |\n",
      "|    policy_loss        | -0.0104  |\n",
      "|    std                | 0.998    |\n",
      "|    value_loss         | 7.29e-06 |\n",
      "------------------------------------\n",
      "Num timesteps: 2000\n",
      "Best mean reward: -77.58 - Last mean reward per episode: -76.41\n",
      "Saving new best model to _lap/best_model.zip\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1e+03    |\n",
      "|    ep_rew_mean        | -62.1    |\n",
      "| time/                 |          |\n",
      "|    fps                | 25       |\n",
      "|    iterations         | 400      |\n",
      "|    time_elapsed       | 78       |\n",
      "|    total_timesteps    | 2000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -4.24    |\n",
      "|    explained_variance | nan      |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 399      |\n",
      "|    policy_loss        | 4.64e-07 |\n",
      "|    std                | 0.995    |\n",
      "|    value_loss         | 1.82e-13 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1e+03     |\n",
      "|    ep_rew_mean        | -62.1     |\n",
      "| time/                 |           |\n",
      "|    fps                | 23        |\n",
      "|    iterations         | 500       |\n",
      "|    time_elapsed       | 104       |\n",
      "|    total_timesteps    | 2500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -4.25     |\n",
      "|    explained_variance | -9.54e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 499       |\n",
      "|    policy_loss        | -0.0306   |\n",
      "|    std                | 0.997     |\n",
      "|    value_loss         | 5.03e-05  |\n",
      "-------------------------------------\n",
      "Num timesteps: 3000\n",
      "Best mean reward: -76.41 - Last mean reward per episode: -74.36\n",
      "Saving new best model to _lap/best_model.zip\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1e+03    |\n",
      "|    ep_rew_mean        | -53.9    |\n",
      "| time/                 |          |\n",
      "|    fps                | 23       |\n",
      "|    iterations         | 600      |\n",
      "|    time_elapsed       | 129      |\n",
      "|    total_timesteps    | 3000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -4.23    |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 599      |\n",
      "|    policy_loss        | -0.0166  |\n",
      "|    std                | 0.991    |\n",
      "|    value_loss         | 2.06e-05 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1e+03    |\n",
      "|    ep_rew_mean        | -53.9    |\n",
      "| time/                 |          |\n",
      "|    fps                | 23       |\n",
      "|    iterations         | 700      |\n",
      "|    time_elapsed       | 151      |\n",
      "|    total_timesteps    | 3500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -4.2     |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 699      |\n",
      "|    policy_loss        | -0.0246  |\n",
      "|    std                | 0.98     |\n",
      "|    value_loss         | 5.09e-05 |\n",
      "------------------------------------\n",
      "Num timesteps: 4000\n",
      "Best mean reward: -74.36 - Last mean reward per episode: -73.62\n",
      "Saving new best model to _lap/best_model.zip\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1e+03    |\n",
      "|    ep_rew_mean        | -55.3    |\n",
      "| time/                 |          |\n",
      "|    fps                | 23       |\n",
      "|    iterations         | 800      |\n",
      "|    time_elapsed       | 172      |\n",
      "|    total_timesteps    | 4000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -4.2     |\n",
      "|    explained_variance | nan      |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 799      |\n",
      "|    policy_loss        | 1.4e-06  |\n",
      "|    std                | 0.98     |\n",
      "|    value_loss         | 0        |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1e+03     |\n",
      "|    ep_rew_mean        | -55.3     |\n",
      "| time/                 |           |\n",
      "|    fps                | 23        |\n",
      "|    iterations         | 900       |\n",
      "|    time_elapsed       | 192       |\n",
      "|    total_timesteps    | 4500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -4.18     |\n",
      "|    explained_variance | 0.00135   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 899       |\n",
      "|    policy_loss        | -8.12e-05 |\n",
      "|    std                | 0.973     |\n",
      "|    value_loss         | 5.64e-10  |\n",
      "-------------------------------------\n",
      "Num timesteps: 5000\n",
      "Best mean reward: -73.62 - Last mean reward per episode: -73.55\n",
      "Saving new best model to _lap/best_model.zip\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1e+03    |\n",
      "|    ep_rew_mean        | -58.7    |\n",
      "| time/                 |          |\n",
      "|    fps                | 23       |\n",
      "|    iterations         | 1000     |\n",
      "|    time_elapsed       | 211      |\n",
      "|    total_timesteps    | 5000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -4.18    |\n",
      "|    explained_variance | nan      |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 999      |\n",
      "|    policy_loss        | 1.76e-06 |\n",
      "|    std                | 0.973    |\n",
      "|    value_loss         | 0        |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1e+03     |\n",
      "|    ep_rew_mean        | -58.7     |\n",
      "| time/                 |           |\n",
      "|    fps                | 23        |\n",
      "|    iterations         | 1100      |\n",
      "|    time_elapsed       | 232       |\n",
      "|    total_timesteps    | 5500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -4.15     |\n",
      "|    explained_variance | 0.000135  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1099      |\n",
      "|    policy_loss        | -0.000279 |\n",
      "|    std                | 0.964     |\n",
      "|    value_loss         | 6e-09     |\n",
      "-------------------------------------\n",
      "Num timesteps: 6000\n",
      "Best mean reward: -73.55 - Last mean reward per episode: -73.14\n",
      "Saving new best model to _lap/best_model.zip\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1e+03    |\n",
      "|    ep_rew_mean        | -59.6    |\n",
      "| time/                 |          |\n",
      "|    fps                | 23       |\n",
      "|    iterations         | 1200     |\n",
      "|    time_elapsed       | 255      |\n",
      "|    total_timesteps    | 6000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -4.15    |\n",
      "|    explained_variance | nan      |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1199     |\n",
      "|    policy_loss        | 1.18e-06 |\n",
      "|    std                | 0.964    |\n",
      "|    value_loss         | 0        |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1e+03     |\n",
      "|    ep_rew_mean        | -59.6     |\n",
      "| time/                 |           |\n",
      "|    fps                | 23        |\n",
      "|    iterations         | 1300      |\n",
      "|    time_elapsed       | 278       |\n",
      "|    total_timesteps    | 6500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -4.16     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1299      |\n",
      "|    policy_loss        | -0.0355   |\n",
      "|    std                | 0.969     |\n",
      "|    value_loss         | 6.32e-05  |\n",
      "-------------------------------------\n",
      "Num timesteps: 7000\n",
      "Best mean reward: -73.14 - Last mean reward per episode: -72.46\n",
      "Saving new best model to _lap/best_model.zip\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1e+03    |\n",
      "|    ep_rew_mean        | -59.4    |\n",
      "| time/                 |          |\n",
      "|    fps                | 23       |\n",
      "|    iterations         | 1400     |\n",
      "|    time_elapsed       | 300      |\n",
      "|    total_timesteps    | 7000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -4.16    |\n",
      "|    explained_variance | nan      |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1399     |\n",
      "|    policy_loss        | 1.53e-06 |\n",
      "|    std                | 0.968    |\n",
      "|    value_loss         | 0        |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1e+03    |\n",
      "|    ep_rew_mean        | -59.4    |\n",
      "| time/                 |          |\n",
      "|    fps                | 23       |\n",
      "|    iterations         | 1500     |\n",
      "|    time_elapsed       | 322      |\n",
      "|    total_timesteps    | 7500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -4.14    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1499     |\n",
      "|    policy_loss        | 3.95     |\n",
      "|    std                | 0.958    |\n",
      "|    value_loss         | 4.02     |\n",
      "------------------------------------\n",
      "Num timesteps: 8000\n",
      "Best mean reward: -72.46 - Last mean reward per episode: -71.49\n",
      "Saving new best model to _lap/best_model.zip\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1e+03     |\n",
      "|    ep_rew_mean        | -58.1     |\n",
      "| time/                 |           |\n",
      "|    fps                | 23        |\n",
      "|    iterations         | 1600      |\n",
      "|    time_elapsed       | 346       |\n",
      "|    total_timesteps    | 8000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -4.11     |\n",
      "|    explained_variance | -0.000235 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1599      |\n",
      "|    policy_loss        | -0.0226   |\n",
      "|    std                | 0.953     |\n",
      "|    value_loss         | 2.92e-05  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1e+03    |\n",
      "|    ep_rew_mean        | -58.1    |\n",
      "| time/                 |          |\n",
      "|    fps                | 23       |\n",
      "|    iterations         | 1700     |\n",
      "|    time_elapsed       | 367      |\n",
      "|    total_timesteps    | 8500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -4.12    |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1699     |\n",
      "|    policy_loss        | -0.0113  |\n",
      "|    std                | 0.956    |\n",
      "|    value_loss         | 1.32e-05 |\n",
      "------------------------------------\n",
      "Num timesteps: 9000\n",
      "Best mean reward: -71.49 - Last mean reward per episode: -70.07\n",
      "Saving new best model to _lap/best_model.zip\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1e+03     |\n",
      "|    ep_rew_mean        | -55.6     |\n",
      "| time/                 |           |\n",
      "|    fps                | 23        |\n",
      "|    iterations         | 1800      |\n",
      "|    time_elapsed       | 388       |\n",
      "|    total_timesteps    | 9000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -4.13     |\n",
      "|    explained_variance | 0.00807   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1799      |\n",
      "|    policy_loss        | -5.97e-05 |\n",
      "|    std                | 0.958     |\n",
      "|    value_loss         | 2.29e-10  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1e+03    |\n",
      "|    ep_rew_mean        | -55.6    |\n",
      "| time/                 |          |\n",
      "|    fps                | 23       |\n",
      "|    iterations         | 1900     |\n",
      "|    time_elapsed       | 410      |\n",
      "|    total_timesteps    | 9500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -4.13    |\n",
      "|    explained_variance | nan      |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1899     |\n",
      "|    policy_loss        | 1.6e-06  |\n",
      "|    std                | 0.958    |\n",
      "|    value_loss         | 0        |\n",
      "------------------------------------\n",
      "Num timesteps: 10000\n",
      "Best mean reward: -70.07 - Last mean reward per episode: -70.41\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1e+03    |\n",
      "|    ep_rew_mean        | -57.9    |\n",
      "| time/                 |          |\n",
      "|    fps                | 23       |\n",
      "|    iterations         | 2000     |\n",
      "|    time_elapsed       | 432      |\n",
      "|    total_timesteps    | 10000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -4.13    |\n",
      "|    explained_variance | nan      |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1999     |\n",
      "|    policy_loss        | 1.4e-06  |\n",
      "|    std                | 0.958    |\n",
      "|    value_loss         | 0        |\n",
      "------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.a2c.a2c.A2C at 0x1e8242e7130>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate the agent\n",
    "model = A2C(\"MlpPolicy\", env, verbose=1)\n",
    "# Train the agent and display a progress bar\n",
    "model.learn(total_timesteps=int(10000),callback=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_reward, std_reward = evaluate_policy(model, model.get_env(), n_eval_episodes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-83.15073220000001 0.6707502729130722\n"
     ]
    }
   ],
   "source": [
    "print(mean_reward, std_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper from the library\n",
    "#results_plotter.plot_results(\n",
    " #   [log_dir], timesteps, results_plotter.X_TIMESTEPS, \"A2C CarRacing-v2\"\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_results(log_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    }
   ],
   "source": [
    "test_env = gym.make(\"CarRacing-v2\", render_mode='human',)# continuous=False)\n",
    "dir = os.path.join(log_dir, \"best_model.zip\")\n",
    "model = A2C.load(dir, env=test_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_env = model.get_env()\n",
    "obs = vec_env.reset()\n",
    "while True:\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, done, info = vec_env.step(action)\n",
    "    #obs, rewards, dones, info = test_env.step(action)\n",
    "    vec_env.render()\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "vec_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    }
   ],
   "source": [
    "test_env = gym.make(\"CarRacing-v2\", render_mode='rgb_array')\n",
    "dir = os.path.join(log_dir, \"best_model\")\n",
    "create_gif(test_env, dir, log_dir, \"a2c_car\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train with CNN instead of MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CarRacing-v2\", render_mode = \"rgb_array\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup callback\n",
    "\n",
    "# Create log dir\n",
    "log_dir = \"a2c_cnn/\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# Wrap the environment\n",
    "env = Monitor(env, log_dir)\n",
    "\n",
    "callback = SaveOnBestTrainingRewardCallback(check_freq=1000, log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 31       |\n",
      "|    iterations         | 100      |\n",
      "|    time_elapsed       | 15       |\n",
      "|    total_timesteps    | 500      |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -4.26    |\n",
      "|    explained_variance | 0.759    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 99       |\n",
      "|    policy_loss        | 0.0258   |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 9.12e-05 |\n",
      "------------------------------------\n",
      "Num timesteps: 1000\n",
      "Best mean reward: -80.89 - Last mean reward per episode: -81.92\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1e+03    |\n",
      "|    ep_rew_mean        | -74.2    |\n",
      "| time/                 |          |\n",
      "|    fps                | 28       |\n",
      "|    iterations         | 200      |\n",
      "|    time_elapsed       | 34       |\n",
      "|    total_timesteps    | 1000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -4.28    |\n",
      "|    explained_variance | 0.287    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 199      |\n",
      "|    policy_loss        | -0.138   |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 0.00169  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1e+03    |\n",
      "|    ep_rew_mean        | -74.2    |\n",
      "| time/                 |          |\n",
      "|    fps                | 26       |\n",
      "|    iterations         | 300      |\n",
      "|    time_elapsed       | 56       |\n",
      "|    total_timesteps    | 1500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -4.31    |\n",
      "|    explained_variance | -141     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 299      |\n",
      "|    policy_loss        | -0.327   |\n",
      "|    std                | 1.02     |\n",
      "|    value_loss         | 0.00745  |\n",
      "------------------------------------\n",
      "Num timesteps: 2000\n",
      "Best mean reward: -80.89 - Last mean reward per episode: -80.62\n",
      "Saving new best model to a2c_cnn/best_model.zip\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1e+03    |\n",
      "|    ep_rew_mean        | -67      |\n",
      "| time/                 |          |\n",
      "|    fps                | 25       |\n",
      "|    iterations         | 400      |\n",
      "|    time_elapsed       | 78       |\n",
      "|    total_timesteps    | 2000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -4.34    |\n",
      "|    explained_variance | 0.432    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 399      |\n",
      "|    policy_loss        | -0.146   |\n",
      "|    std                | 1.03     |\n",
      "|    value_loss         | 0.000875 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1e+03    |\n",
      "|    ep_rew_mean        | -67      |\n",
      "| time/                 |          |\n",
      "|    fps                | 24       |\n",
      "|    iterations         | 500      |\n",
      "|    time_elapsed       | 100      |\n",
      "|    total_timesteps    | 2500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -4.33    |\n",
      "|    explained_variance | 0.253    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 499      |\n",
      "|    policy_loss        | 0.195    |\n",
      "|    std                | 1.02     |\n",
      "|    value_loss         | 0.00237  |\n",
      "------------------------------------\n",
      "Num timesteps: 3000\n",
      "Best mean reward: -80.62 - Last mean reward per episode: -80.52\n",
      "Saving new best model to a2c_cnn/best_model.zip\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1e+03    |\n",
      "|    ep_rew_mean        | -70.9    |\n",
      "| time/                 |          |\n",
      "|    fps                | 24       |\n",
      "|    iterations         | 600      |\n",
      "|    time_elapsed       | 122      |\n",
      "|    total_timesteps    | 3000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -4.33    |\n",
      "|    explained_variance | -6.19    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 599      |\n",
      "|    policy_loss        | 0.112    |\n",
      "|    std                | 1.03     |\n",
      "|    value_loss         | 0.00075  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1e+03    |\n",
      "|    ep_rew_mean        | -70.9    |\n",
      "| time/                 |          |\n",
      "|    fps                | 24       |\n",
      "|    iterations         | 700      |\n",
      "|    time_elapsed       | 145      |\n",
      "|    total_timesteps    | 3500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -4.33    |\n",
      "|    explained_variance | 0.0224   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 699      |\n",
      "|    policy_loss        | -0.0598  |\n",
      "|    std                | 1.03     |\n",
      "|    value_loss         | 0.000277 |\n",
      "------------------------------------\n",
      "Num timesteps: 4000\n",
      "Best mean reward: -80.52 - Last mean reward per episode: -80.63\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1e+03    |\n",
      "|    ep_rew_mean        | -73.8    |\n",
      "| time/                 |          |\n",
      "|    fps                | 23       |\n",
      "|    iterations         | 800      |\n",
      "|    time_elapsed       | 169      |\n",
      "|    total_timesteps    | 4000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -4.33    |\n",
      "|    explained_variance | 0.242    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 799      |\n",
      "|    policy_loss        | -0.104   |\n",
      "|    std                | 1.03     |\n",
      "|    value_loss         | 0.000909 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1e+03    |\n",
      "|    ep_rew_mean        | -73.8    |\n",
      "| time/                 |          |\n",
      "|    fps                | 23       |\n",
      "|    iterations         | 900      |\n",
      "|    time_elapsed       | 194      |\n",
      "|    total_timesteps    | 4500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -4.31    |\n",
      "|    explained_variance | -16.8    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 899      |\n",
      "|    policy_loss        | -0.131   |\n",
      "|    std                | 1.02     |\n",
      "|    value_loss         | 0.00109  |\n",
      "------------------------------------\n",
      "Num timesteps: 5000\n",
      "Best mean reward: -80.52 - Last mean reward per episode: -80.53\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1e+03    |\n",
      "|    ep_rew_mean        | -74.8    |\n",
      "| time/                 |          |\n",
      "|    fps                | 23       |\n",
      "|    iterations         | 1000     |\n",
      "|    time_elapsed       | 213      |\n",
      "|    total_timesteps    | 5000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -4.31    |\n",
      "|    explained_variance | 0.847    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 999      |\n",
      "|    policy_loss        | -0.0642  |\n",
      "|    std                | 1.02     |\n",
      "|    value_loss         | 0.000297 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1e+03    |\n",
      "|    ep_rew_mean        | -74.8    |\n",
      "| time/                 |          |\n",
      "|    fps                | 23       |\n",
      "|    iterations         | 1100     |\n",
      "|    time_elapsed       | 234      |\n",
      "|    total_timesteps    | 5500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -4.31    |\n",
      "|    explained_variance | 0.672    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1099     |\n",
      "|    policy_loss        | -0.0767  |\n",
      "|    std                | 1.02     |\n",
      "|    value_loss         | 0.000509 |\n",
      "------------------------------------\n",
      "Num timesteps: 6000\n",
      "Best mean reward: -80.52 - Last mean reward per episode: -79.88\n",
      "Saving new best model to a2c_cnn/best_model.zip\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1e+03    |\n",
      "|    ep_rew_mean        | -73.5    |\n",
      "| time/                 |          |\n",
      "|    fps                | 23       |\n",
      "|    iterations         | 1200     |\n",
      "|    time_elapsed       | 254      |\n",
      "|    total_timesteps    | 6000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -4.31    |\n",
      "|    explained_variance | 0.108    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1199     |\n",
      "|    policy_loss        | 0.116    |\n",
      "|    std                | 1.02     |\n",
      "|    value_loss         | 0.000717 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1e+03    |\n",
      "|    ep_rew_mean        | -73.5    |\n",
      "| time/                 |          |\n",
      "|    fps                | 23       |\n",
      "|    iterations         | 1300     |\n",
      "|    time_elapsed       | 273      |\n",
      "|    total_timesteps    | 6500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -4.35    |\n",
      "|    explained_variance | -0.331   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1299     |\n",
      "|    policy_loss        | 0.107    |\n",
      "|    std                | 1.03     |\n",
      "|    value_loss         | 0.000797 |\n",
      "------------------------------------\n",
      "Num timesteps: 7000\n",
      "Best mean reward: -79.88 - Last mean reward per episode: -80.13\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1e+03    |\n",
      "|    ep_rew_mean        | -75.2    |\n",
      "| time/                 |          |\n",
      "|    fps                | 23       |\n",
      "|    iterations         | 1400     |\n",
      "|    time_elapsed       | 292      |\n",
      "|    total_timesteps    | 7000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -4.36    |\n",
      "|    explained_variance | 0.253    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1399     |\n",
      "|    policy_loss        | -0.0635  |\n",
      "|    std                | 1.03     |\n",
      "|    value_loss         | 0.000252 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1e+03    |\n",
      "|    ep_rew_mean        | -75.2    |\n",
      "| time/                 |          |\n",
      "|    fps                | 23       |\n",
      "|    iterations         | 1500     |\n",
      "|    time_elapsed       | 313      |\n",
      "|    total_timesteps    | 7500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -4.37    |\n",
      "|    explained_variance | 0.105    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1499     |\n",
      "|    policy_loss        | 0.0762   |\n",
      "|    std                | 1.04     |\n",
      "|    value_loss         | 0.000352 |\n",
      "------------------------------------\n",
      "Num timesteps: 8000\n",
      "Best mean reward: -79.88 - Last mean reward per episode: -80.68\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1e+03    |\n",
      "|    ep_rew_mean        | -77.4    |\n",
      "| time/                 |          |\n",
      "|    fps                | 24       |\n",
      "|    iterations         | 1600     |\n",
      "|    time_elapsed       | 332      |\n",
      "|    total_timesteps    | 8000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -4.37    |\n",
      "|    explained_variance | 0.136    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1599     |\n",
      "|    policy_loss        | 0.0832   |\n",
      "|    std                | 1.04     |\n",
      "|    value_loss         | 0.000471 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1e+03     |\n",
      "|    ep_rew_mean        | -77.4     |\n",
      "| time/                 |           |\n",
      "|    fps                | 23        |\n",
      "|    iterations         | 1700      |\n",
      "|    time_elapsed       | 356       |\n",
      "|    total_timesteps    | 8500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -4.37     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1699      |\n",
      "|    policy_loss        | 0.0621    |\n",
      "|    std                | 1.04      |\n",
      "|    value_loss         | 0.000256  |\n",
      "-------------------------------------\n",
      "Num timesteps: 9000\n",
      "Best mean reward: -79.88 - Last mean reward per episode: -81.20\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1e+03    |\n",
      "|    ep_rew_mean        | -79.1    |\n",
      "| time/                 |          |\n",
      "|    fps                | 23       |\n",
      "|    iterations         | 1800     |\n",
      "|    time_elapsed       | 379      |\n",
      "|    total_timesteps    | 9000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -4.35    |\n",
      "|    explained_variance | -0.0382  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1799     |\n",
      "|    policy_loss        | 0.0907   |\n",
      "|    std                | 1.03     |\n",
      "|    value_loss         | 0.000524 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1e+03    |\n",
      "|    ep_rew_mean        | -79.1    |\n",
      "| time/                 |          |\n",
      "|    fps                | 23       |\n",
      "|    iterations         | 1900     |\n",
      "|    time_elapsed       | 398      |\n",
      "|    total_timesteps    | 9500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -4.37    |\n",
      "|    explained_variance | 0.118    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1899     |\n",
      "|    policy_loss        | -0.117   |\n",
      "|    std                | 1.04     |\n",
      "|    value_loss         | 0.000517 |\n",
      "------------------------------------\n",
      "Num timesteps: 10000\n",
      "Best mean reward: -79.88 - Last mean reward per episode: -81.48\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1e+03    |\n",
      "|    ep_rew_mean        | -80      |\n",
      "| time/                 |          |\n",
      "|    fps                | 23       |\n",
      "|    iterations         | 2000     |\n",
      "|    time_elapsed       | 422      |\n",
      "|    total_timesteps    | 10000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -4.38    |\n",
      "|    explained_variance | 0.0368   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1999     |\n",
      "|    policy_loss        | -0.0851  |\n",
      "|    std                | 1.04     |\n",
      "|    value_loss         | 0.000295 |\n",
      "------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.a2c.a2c.A2C at 0x1e83eba5df0>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate the agent\n",
    "model = A2C(\"CnnPolicy\", env, verbose=1)\n",
    "# Train the agent and display a progress bar\n",
    "timesteps = 10000\n",
    "model.learn(total_timesteps=int(timesteps), callback=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-93.20874360000002 0.6813822410663796\n"
     ]
    }
   ],
   "source": [
    "mean_reward, std_reward = evaluate_policy(model, model.get_env(), n_eval_episodes=5)\n",
    "\n",
    "print(mean_reward, std_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Helper from the library\n",
    "#results_plotter.plot_results(\n",
    " #   [log_dir], timesteps, results_plotter.X_TIMESTEPS, \"A2C CarRacing-v2\"\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot_results(log_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    }
   ],
   "source": [
    "test_env = gym.make(\"CarRacing-v2\", render_mode='human')\n",
    "dir = os.path.join(log_dir, \"best_model.zip\")\n",
    "model = A2C.load(dir, env=test_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_env = model.get_env()\n",
    "obs = vec_env.reset()\n",
    "while True:\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, done, info = vec_env.step(action)\n",
    "    #obs, rewards, dones, info = test_env.step(action)\n",
    "    vec_env.render()\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "vec_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    }
   ],
   "source": [
    "test_env = gym.make(\"CarRacing-v2\", render_mode='rgb_array')\n",
    "dir = os.path.join(log_dir, \"best_model\")\n",
    "create_gif(test_env, dir, log_dir, \"a2c_car\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

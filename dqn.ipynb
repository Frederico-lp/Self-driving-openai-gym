{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as trans\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from dqn_model import CNN\n",
    "from replay_memory import ReplayMemory, Transition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CarRacing-v2\")\n",
    "\n",
    "for i_episode in range(NUM_EPISODES):\n",
    "    episode_losses = []\n",
    "    total_reward = 0\n",
    "    RANDOM_COUNTER = 0\n",
    "    NN_COUNTER = 0\n",
    "    score_sliding = 0\n",
    "\n",
    "    # Initialize the environment and state\n",
    "    print(\"Starting episode\", i_episode)\n",
    "    env.reset()\n",
    "    wait_for_zoom()\n",
    "    state, _, _ = get_res_state([0, 0, 0])\n",
    "\n",
    "    for t in range(10000):\n",
    "        # select current action\n",
    "        action_index = select_action(state)\n",
    "        action = ACTIONSPACE[action_index]\n",
    "\n",
    "        # get normalized state from current action\n",
    "        next_state, reward, done = get_res_state(action)\n",
    "\n",
    "        total_reward += reward\n",
    "\n",
    "        reward = float(reward)\n",
    "        reward = torch.tensor([reward], device=DEVICE)\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "        # Store the transition in memory\n",
    "        MEMORY.push(state, action_index, reward, next_state, done)\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        # Perform one step of the optimization (on the target network)\n",
    "        episode_loss = optimize_model()\n",
    "\n",
    "        if episode_loss is not None:\n",
    "            episode_losses.append(episode_loss)\n",
    "\n",
    "    if len(episode_losses) > 0:\n",
    "        avgloss = sum(episode_losses)/len(episode_losses)\n",
    "\n",
    "    else:\n",
    "        avgloss = 0\n",
    "\n",
    "    TOTAL_AVG_LOSSES.append(avgloss)\n",
    "\n",
    "    print(f\"Random steps this episode {RANDOM_COUNTER}\")\n",
    "    print(f\"NN steps this episode {NN_COUNTER}\")\n",
    "    print(f\"Total reward in episode {i_episode} is {total_reward} and awg loss is {avgloss}\")\n",
    "    TOTAL_SCORES.append(total_reward)\n",
    "    score_sliding = sum(TOTAL_SCORES) / len(TOTAL_SCORES)\n",
    "    SCORE_AVG.append(score_sliding)\n",
    "\n",
    "# show graphs\n",
    "plt.plot(TOTAL_SCORES)\n",
    "plt.plot(SCORE_AVG)\n",
    "plt.title(f\"Scores per episode, batch size {BATCH_SIZE}\")\n",
    "plt.show()\n",
    "plt.plot(TOTAL_AVG_LOSSES)\n",
    "plt.title(f\"Avg losses per episode\")\n",
    "plt.show()\n",
    "\n",
    "print('Complete')\n",
    "\n",
    "env.close()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

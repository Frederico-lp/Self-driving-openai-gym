{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gymnasium as gym\n",
    "\n",
    "from stable_baselines3 import SAC\n",
    "from stable_baselines3.common import results_plotter\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.results_plotter import load_results, ts2xy\n",
    "from stable_baselines3.common.monitor import Monitor\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveOnBestTrainingRewardCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    Callback for saving a model (the check is done every ``check_freq`` steps)\n",
    "    based on the training reward (in practice, we recommend using ``EvalCallback``).\n",
    "\n",
    "    :param check_freq: (int)\n",
    "    :param log_dir: (str) Path to the folder where the model will be saved.\n",
    "      It must contains the file created by the ``Monitor`` wrapper.\n",
    "    :param verbose: (int)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, check_freq: int, log_dir: str, verbose=1):\n",
    "        super().__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.log_dir = log_dir\n",
    "        self.save_path = os.path.join(log_dir, \"best_model\")\n",
    "        self.best_mean_reward = -np.inf\n",
    "\n",
    "    # def _init_callback(self) -> None:\n",
    "    #     # Create folder if needed\n",
    "    #     if self.save_path is not None:\n",
    "    #         os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "\n",
    "            # Retrieve training reward\n",
    "            x, y = ts2xy(load_results(self.log_dir), \"timesteps\")\n",
    "            if len(x) > 0:\n",
    "                # Mean training reward over the last 100 episodes\n",
    "                mean_reward = np.mean(y[-100:])\n",
    "                if self.verbose > 0:\n",
    "                    print(f\"Num timesteps: {self.num_timesteps}\")\n",
    "                    print(\n",
    "                        f\"Best mean reward: {self.best_mean_reward:.2f} - Last mean reward per episode: {mean_reward:.2f}\"\n",
    "                    )\n",
    "\n",
    "                # New best model, you could save the agent here\n",
    "                if mean_reward > self.best_mean_reward:\n",
    "                    self.best_mean_reward = mean_reward\n",
    "                    # Example for saving best model\n",
    "                    if self.verbose > 0:\n",
    "                        print(f\"Saving new best model to {self.save_path}.zip\")\n",
    "                    self.model.save(self.save_path)\n",
    "\n",
    "        return True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(values, window):\n",
    "    \"\"\"\n",
    "    Smooth values by doing a moving average\n",
    "    :param values: (numpy array)\n",
    "    :param window: (int)\n",
    "    :return: (numpy array)\n",
    "    \"\"\"\n",
    "    weights = np.repeat(1.0, window) / window\n",
    "    return np.convolve(values, weights, \"valid\")\n",
    "\n",
    "\n",
    "def plot_results(log_folder, title=\"Learning Curve\"):\n",
    "    \"\"\"\n",
    "    plot the results\n",
    "\n",
    "    :param log_folder: (str) the save location of the results to plot\n",
    "    :param title: (str) the title of the task to plot\n",
    "    \"\"\"\n",
    "    x, y = ts2xy(load_results(log_folder), \"timesteps\")\n",
    "    #y = moving_average(y, window=50)\n",
    "    # Truncate x\n",
    "    #x = x[len(x) - len(y) :]\n",
    "\n",
    "    fig = plt.figure(title)\n",
    "    plt.plot(x, y)\n",
    "    plt.xlabel(\"Number of Timesteps\")\n",
    "    plt.ylabel(\"Rewards\")\n",
    "    plt.title(title + \" Smoothed\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Gif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "import numpy as np\n",
    "#from stable_baselines3 import A2C\n",
    "\n",
    "def create_gif(env, model_path, path, name):\n",
    "    #model = A2C.load(model_path, env=env)\n",
    "    #load model\n",
    "    model = SAC.load(model_path, env=env)\n",
    "    images = []\n",
    "\n",
    "    vec_env = model.get_env()\n",
    "    obs = vec_env.reset()\n",
    "    img = vec_env.render()\n",
    "    for i in range(1000):\n",
    "        images.append(img)\n",
    "        action, _ = model.predict(obs)\n",
    "        obs, _, _ ,_ = vec_env.step(action)\n",
    "        img = vec_env.render(mode='rgb_array')\n",
    "    gif_name =  path + name + '.gif'\n",
    "    imageio.mimsave(gif_name, [np.array(img) for i, img in enumerate(images) if i%2 == 0], duration=90)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment\n",
    "env = gym.make(\"CarRacing-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup callback\n",
    "\n",
    "# Create log dir\n",
    "log_dir = \"sac_normal/\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# Wrap the environment\n",
    "env = Monitor(env, log_dir)\n",
    "\n",
    "callback = SaveOnBestTrainingRewardCallback(check_freq=1000, log_dir=log_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "Num timesteps: 1000\n",
      "Best mean reward: -inf - Last mean reward per episode: -40.81\n",
      "Saving new best model to sac_normal/best_model.zip\n",
      "Num timesteps: 2000\n",
      "Best mean reward: -40.81 - Last mean reward per episode: -38.66\n",
      "Saving new best model to sac_normal/best_model.zip\n",
      "Num timesteps: 3000\n",
      "Best mean reward: -38.66 - Last mean reward per episode: -37.42\n",
      "Saving new best model to sac_normal/best_model.zip\n",
      "Num timesteps: 4000\n",
      "Best mean reward: -37.42 - Last mean reward per episode: -36.10\n",
      "Saving new best model to sac_normal/best_model.zip\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -36.1    |\n",
      "| time/              |          |\n",
      "|    episodes        | 4        |\n",
      "|    fps             | 0        |\n",
      "|    time_elapsed    | 5115     |\n",
      "|    total_timesteps | 4000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -19.8    |\n",
      "|    critic_loss     | 0.0721   |\n",
      "|    ent_coef        | 0.31     |\n",
      "|    ent_coef_loss   | -5.93    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3899     |\n",
      "---------------------------------\n",
      "Num timesteps: 5000\n",
      "Best mean reward: -36.10 - Last mean reward per episode: -35.93\n",
      "Saving new best model to sac_normal/best_model.zip\n",
      "Num timesteps: 6000\n",
      "Best mean reward: -35.93 - Last mean reward per episode: -36.89\n",
      "Num timesteps: 7000\n",
      "Best mean reward: -35.93 - Last mean reward per episode: -35.13\n",
      "Saving new best model to sac_normal/best_model.zip\n",
      "Num timesteps: 8000\n",
      "Best mean reward: -35.13 - Last mean reward per episode: -35.74\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -35.7    |\n",
      "| time/              |          |\n",
      "|    episodes        | 8        |\n",
      "|    fps             | 0        |\n",
      "|    time_elapsed    | 10912    |\n",
      "|    total_timesteps | 8000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -21.3    |\n",
      "|    critic_loss     | 0.248    |\n",
      "|    ent_coef        | 0.0936   |\n",
      "|    ent_coef_loss   | -11.8    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7899     |\n",
      "---------------------------------\n",
      "Num timesteps: 9000\n",
      "Best mean reward: -35.13 - Last mean reward per episode: -33.72\n",
      "Saving new best model to sac_normal/best_model.zip\n",
      "Num timesteps: 10000\n",
      "Best mean reward: -33.72 - Last mean reward per episode: -33.57\n",
      "Saving new best model to sac_normal/best_model.zip\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.sac.sac.SAC at 0x28290463e20>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate the agent\n",
    "model = SAC(\"MlpPolicy\", env, verbose=1, device='cuda', buffer_size=10000)\n",
    "# Train the agent and display a progress bar\n",
    "timesteps = 10000\n",
    "model.learn(total_timesteps=int(timesteps), callback=callback)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_reward, std_reward = evaluate_policy(model, model.get_env(), n_eval_episodes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-24.2725584 5.825913801719336\n"
     ]
    }
   ],
   "source": [
    "print(mean_reward, std_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper from the library\n",
    "#results_plotter.plot_results(\n",
    "#    [log_dir], timesteps, results_plotter.X_TIMESTEPS, \"sac CarRacing-v2\"\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_results(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    " #model.save(\"sac_normal\")\n",
    "# create_gif(env, \"sac_normal/best_model.zip\", \"sac_normal/\", \"sac_normal\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    }
   ],
   "source": [
    "test_env = gym.make(\"CarRacing-v2\", render_mode='human')\n",
    "dir = os.path.join(log_dir, \"best_model.zip\")\n",
    "model = SAC.load(dir, env=test_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_env = model.get_env()\n",
    "obs = vec_env.reset()\n",
    "while True:\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, done, info = vec_env.step(action)\n",
    "    #obs, rewards, dones, info = test_env.step(action)\n",
    "    vec_env.render()\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "vec_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    }
   ],
   "source": [
    "test_env = gym.make(\"CarRacing-v2\", render_mode='rgb_array')\n",
    "dir = os.path.join(log_dir, \"best_model.zip\")\n",
    "create_gif(test_env, dir, log_dir, \"sac_car\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discrete space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment\n",
    "env = gym.make(\"CarRacing-v2\", continuous=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup callback\n",
    "\n",
    "# Create log dir\n",
    "log_dir = \"sac_discrete/\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# Wrap the environment\n",
    "env = Monitor(env, log_dir)\n",
    "\n",
    "callback = SaveOnBestTrainingRewardCallback(check_freq=1000, log_dir=log_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "The algorithm only supports (<class 'gymnasium.spaces.box.Box'>,) as action spaces but Discrete(5) was provided",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Instantiate the agent\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m model \u001b[39m=\u001b[39m SAC(\u001b[39m\"\u001b[39;49m\u001b[39mMlpPolicy\u001b[39;49m\u001b[39m\"\u001b[39;49m, env, verbose\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, buffer_size\u001b[39m=\u001b[39;49m\u001b[39m10000\u001b[39;49m)\n\u001b[0;32m      3\u001b[0m \u001b[39m# Train the agent and display a progress bar\u001b[39;00m\n\u001b[0;32m      4\u001b[0m timesteps \u001b[39m=\u001b[39m \u001b[39m10000\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\candidomariz\\AppData\\Local\\miniconda3\\lib\\site-packages\\stable_baselines3\\sac\\sac.py:120\u001b[0m, in \u001b[0;36mSAC.__init__\u001b[1;34m(self, policy, env, learning_rate, buffer_size, learning_starts, batch_size, tau, gamma, train_freq, gradient_steps, action_noise, replay_buffer_class, replay_buffer_kwargs, optimize_memory_usage, ent_coef, target_update_interval, target_entropy, use_sde, sde_sample_freq, use_sde_at_warmup, stats_window_size, tensorboard_log, policy_kwargs, verbose, seed, device, _init_setup_model)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[0;32m     91\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m     92\u001b[0m     policy: Union[\u001b[39mstr\u001b[39m, Type[SACPolicy]],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    118\u001b[0m     _init_setup_model: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m    119\u001b[0m ):\n\u001b[1;32m--> 120\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[0;32m    121\u001b[0m         policy,\n\u001b[0;32m    122\u001b[0m         env,\n\u001b[0;32m    123\u001b[0m         learning_rate,\n\u001b[0;32m    124\u001b[0m         buffer_size,\n\u001b[0;32m    125\u001b[0m         learning_starts,\n\u001b[0;32m    126\u001b[0m         batch_size,\n\u001b[0;32m    127\u001b[0m         tau,\n\u001b[0;32m    128\u001b[0m         gamma,\n\u001b[0;32m    129\u001b[0m         train_freq,\n\u001b[0;32m    130\u001b[0m         gradient_steps,\n\u001b[0;32m    131\u001b[0m         action_noise,\n\u001b[0;32m    132\u001b[0m         replay_buffer_class\u001b[39m=\u001b[39;49mreplay_buffer_class,\n\u001b[0;32m    133\u001b[0m         replay_buffer_kwargs\u001b[39m=\u001b[39;49mreplay_buffer_kwargs,\n\u001b[0;32m    134\u001b[0m         policy_kwargs\u001b[39m=\u001b[39;49mpolicy_kwargs,\n\u001b[0;32m    135\u001b[0m         stats_window_size\u001b[39m=\u001b[39;49mstats_window_size,\n\u001b[0;32m    136\u001b[0m         tensorboard_log\u001b[39m=\u001b[39;49mtensorboard_log,\n\u001b[0;32m    137\u001b[0m         verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[0;32m    138\u001b[0m         device\u001b[39m=\u001b[39;49mdevice,\n\u001b[0;32m    139\u001b[0m         seed\u001b[39m=\u001b[39;49mseed,\n\u001b[0;32m    140\u001b[0m         use_sde\u001b[39m=\u001b[39;49muse_sde,\n\u001b[0;32m    141\u001b[0m         sde_sample_freq\u001b[39m=\u001b[39;49msde_sample_freq,\n\u001b[0;32m    142\u001b[0m         use_sde_at_warmup\u001b[39m=\u001b[39;49muse_sde_at_warmup,\n\u001b[0;32m    143\u001b[0m         optimize_memory_usage\u001b[39m=\u001b[39;49moptimize_memory_usage,\n\u001b[0;32m    144\u001b[0m         supported_action_spaces\u001b[39m=\u001b[39;49m(spaces\u001b[39m.\u001b[39;49mBox,),\n\u001b[0;32m    145\u001b[0m         support_multi_env\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    146\u001b[0m     )\n\u001b[0;32m    148\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_entropy \u001b[39m=\u001b[39m target_entropy\n\u001b[0;32m    149\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlog_ent_coef \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m  \u001b[39m# type: Optional[th.Tensor]\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\candidomariz\\AppData\\Local\\miniconda3\\lib\\site-packages\\stable_baselines3\\common\\off_policy_algorithm.py:110\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.__init__\u001b[1;34m(self, policy, env, learning_rate, buffer_size, learning_starts, batch_size, tau, gamma, train_freq, gradient_steps, action_noise, replay_buffer_class, replay_buffer_kwargs, optimize_memory_usage, policy_kwargs, stats_window_size, tensorboard_log, verbose, device, support_multi_env, monitor_wrapper, seed, use_sde, sde_sample_freq, use_sde_at_warmup, sde_support, supported_action_spaces)\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[0;32m     81\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m     82\u001b[0m     policy: Union[\u001b[39mstr\u001b[39m, Type[BasePolicy]],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    108\u001b[0m     supported_action_spaces: Optional[Tuple[Type[spaces\u001b[39m.\u001b[39mSpace], \u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    109\u001b[0m ):\n\u001b[1;32m--> 110\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[0;32m    111\u001b[0m         policy\u001b[39m=\u001b[39;49mpolicy,\n\u001b[0;32m    112\u001b[0m         env\u001b[39m=\u001b[39;49menv,\n\u001b[0;32m    113\u001b[0m         learning_rate\u001b[39m=\u001b[39;49mlearning_rate,\n\u001b[0;32m    114\u001b[0m         policy_kwargs\u001b[39m=\u001b[39;49mpolicy_kwargs,\n\u001b[0;32m    115\u001b[0m         stats_window_size\u001b[39m=\u001b[39;49mstats_window_size,\n\u001b[0;32m    116\u001b[0m         tensorboard_log\u001b[39m=\u001b[39;49mtensorboard_log,\n\u001b[0;32m    117\u001b[0m         verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[0;32m    118\u001b[0m         device\u001b[39m=\u001b[39;49mdevice,\n\u001b[0;32m    119\u001b[0m         support_multi_env\u001b[39m=\u001b[39;49msupport_multi_env,\n\u001b[0;32m    120\u001b[0m         monitor_wrapper\u001b[39m=\u001b[39;49mmonitor_wrapper,\n\u001b[0;32m    121\u001b[0m         seed\u001b[39m=\u001b[39;49mseed,\n\u001b[0;32m    122\u001b[0m         use_sde\u001b[39m=\u001b[39;49muse_sde,\n\u001b[0;32m    123\u001b[0m         sde_sample_freq\u001b[39m=\u001b[39;49msde_sample_freq,\n\u001b[0;32m    124\u001b[0m         supported_action_spaces\u001b[39m=\u001b[39;49msupported_action_spaces,\n\u001b[0;32m    125\u001b[0m     )\n\u001b[0;32m    126\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuffer_size \u001b[39m=\u001b[39m buffer_size\n\u001b[0;32m    127\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_size \u001b[39m=\u001b[39m batch_size\n",
      "File \u001b[1;32mc:\\Users\\candidomariz\\AppData\\Local\\miniconda3\\lib\\site-packages\\stable_baselines3\\common\\base_class.py:180\u001b[0m, in \u001b[0;36mBaseAlgorithm.__init__\u001b[1;34m(self, policy, env, learning_rate, policy_kwargs, stats_window_size, tensorboard_log, verbose, device, support_multi_env, monitor_wrapper, seed, use_sde, sde_sample_freq, supported_action_spaces)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_vec_normalize_env \u001b[39m=\u001b[39m unwrap_vec_normalize(env)\n\u001b[0;32m    179\u001b[0m \u001b[39mif\u001b[39;00m supported_action_spaces \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 180\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_space, supported_action_spaces), (\n\u001b[0;32m    181\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe algorithm only supports \u001b[39m\u001b[39m{\u001b[39;00msupported_action_spaces\u001b[39m}\u001b[39;00m\u001b[39m as action spaces \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    182\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_space\u001b[39m}\u001b[39;00m\u001b[39m was provided\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    183\u001b[0m     )\n\u001b[0;32m    185\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m support_multi_env \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_envs \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    186\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    187\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mError: the model does not support multiple envs; it requires \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m\"\u001b[39m\u001b[39ma single vectorized environment.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    188\u001b[0m     )\n",
      "\u001b[1;31mAssertionError\u001b[0m: The algorithm only supports (<class 'gymnasium.spaces.box.Box'>,) as action spaces but Discrete(5) was provided"
     ]
    }
   ],
   "source": [
    "# Instantiate the agent\n",
    "model = SAC(\"MlpPolicy\", env, verbose=1, device='cuda', buffer_size=10000)\n",
    "# Train the agent and display a progress bar\n",
    "timesteps = 10000\n",
    "model.learn(total_timesteps=int(timesteps), callback=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_reward, std_reward = evaluate_policy(model, model.get_env(), n_eval_episodes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mean_reward, std_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper from the library\n",
    "#results_plotter.plot_results(\n",
    "#    [log_dir], timesteps, results_plotter.X_TIMESTEPS, \"sac CarRacing-v2\"\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_results(log_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_env = gym.make(\"CarRacing-v2\", render_mode='human', continuous=False)\n",
    "dir = os.path.join(log_dir, \"best_model.zip\")\n",
    "model = SAC.load(dir, env=test_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_env = model.get_env()\n",
    "obs = vec_env.reset()\n",
    "while True:\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, done, info = vec_env.step(action)\n",
    "    #obs, rewards, dones, info = test_env.step(action)\n",
    "    vec_env.render()\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "vec_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_env = gym.make(\"CarRacing-v2\", render_mode='rgb_array', continuous=False)\n",
    "dir = os.path.join(log_dir, \"best_model\")\n",
    "create_gif(test_env, dir, log_dir, \"sac_car\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lap completion percentage change "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment\n",
    "env = gym.make(\"CarRacing-v2\", lap_complete_percent=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup callback\n",
    "\n",
    "# Create log dir\n",
    "log_dir = \"sac_lap/\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# Wrap the environment\n",
    "env = Monitor(env, log_dir)\n",
    "\n",
    "callback = SaveOnBestTrainingRewardCallback(check_freq=1000, log_dir=log_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "Num timesteps: 1000\n",
      "Best mean reward: -inf - Last mean reward per episode: -37.93\n",
      "Saving new best model to _lap/best_model.zip\n",
      "Num timesteps: 2000\n",
      "Best mean reward: -37.93 - Last mean reward per episode: -31.86\n",
      "Saving new best model to _lap/best_model.zip\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the agent\n",
    "model = SAC(\"MlpPolicy\", env, verbose=1, buffer_size=10000)\n",
    "# Train the agent and display a progress bar\n",
    "timesteps = 10000\n",
    "model.learn(total_timesteps=int(timesteps), callback=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_reward, std_reward = evaluate_policy(model, model.get_env(), n_eval_episodes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mean_reward, std_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper from the library\n",
    "#results_plotter.plot_results(\n",
    "#    [log_dir], timesteps, results_plotter.X_TIMESTEPS, \"sac CarRacing-v2\"\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_results(log_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_env = gym.make(\"CarRacing-v2\", render_mode='human', continuous=False)\n",
    "dir = os.path.join(log_dir, \"best_model.zip\")\n",
    "model = SAC.load(dir, env=test_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_env = model.get_env()\n",
    "obs = vec_env.reset()\n",
    "while True:\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, done, info = vec_env.step(action)\n",
    "    #obs, rewards, dones, info = test_env.step(action)\n",
    "    vec_env.render()\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "vec_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_env = gym.make(\"CarRacing-v2\", render_mode='rgb_array')\n",
    "dir = os.path.join(log_dir, \"best_model\")\n",
    "create_gif(test_env, dir, log_dir, \"sac_car\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train with CNN instead of MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CarRacing-v2\", render_mode = \"rgb_array\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup callback\n",
    "\n",
    "# Create log dir\n",
    "log_dir = \"sac_cnn/\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# Wrap the environment\n",
    "env = Monitor(env, log_dir)\n",
    "\n",
    "callback = SaveOnBestTrainingRewardCallback(check_freq=1000, log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the agent\n",
    "model = SAC(\"CnnPolicy\", env, verbose=1, buffer_size=10000)\n",
    "# Train the agent and display a progress bar\n",
    "timesteps = 10000\n",
    "model.learn(total_timesteps=int(timesteps), callback=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_reward, std_reward = evaluate_policy(model, model.get_env(), n_eval_episodes=5)\n",
    "\n",
    "print(mean_reward, std_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper from the library\n",
    "#results_plotter.plot_results(\n",
    "#    [log_dir], timesteps, results_plotter.X_TIMESTEPS, \"sac CarRacing-v2\"\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_results(log_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'log_dir' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m test_env \u001b[39m=\u001b[39m gym\u001b[39m.\u001b[39mmake(\u001b[39m\"\u001b[39m\u001b[39mCarRacing-v2\u001b[39m\u001b[39m\"\u001b[39m, render_mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mhuman\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m \u001b[39mdir\u001b[39m \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(log_dir, \u001b[39m\"\u001b[39m\u001b[39mbest_model.zip\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m model \u001b[39m=\u001b[39m SAC\u001b[39m.\u001b[39mload(\u001b[39mdir\u001b[39m, env\u001b[39m=\u001b[39mtest_env)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'log_dir' is not defined"
     ]
    }
   ],
   "source": [
    "test_env = gym.make(\"CarRacing-v2\", render_mode='human')\n",
    "dir = os.path.join(log_dir, \"best_model.zip\")\n",
    "model = SAC.load(dir, env=test_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_env = model.get_env()\n",
    "obs = vec_env.reset()\n",
    "while True:\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, done, info = vec_env.step(action)\n",
    "    #obs, rewards, dones, info = test_env.step(action)\n",
    "    vec_env.render()\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "vec_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_env = gym.make(\"CarRacing-v2\", render_mode='rgb_array')\n",
    "dir = os.path.join(log_dir, \"best_model\")\n",
    "create_gif(test_env, dir, log_dir, \"sac_car\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
